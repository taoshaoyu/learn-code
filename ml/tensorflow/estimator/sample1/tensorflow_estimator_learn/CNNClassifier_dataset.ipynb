{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 那些事儿之DL中的 HELLO WORLD\n",
    "\n",
    "- 基于MNIST数据集，运用TensorFlow中的 **tf.estimator** 中的 **tf.estimator.Estimator** 搭建一个简单的卷积神经网络，实现模型的训练，验证和测试\n",
    "\n",
    "- TensorBoard的简单使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入各个库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from tensorflow import data\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST数据集载入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看MNIST数据长什么样子的\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST数据集包含70000张图像和对应的标签（图像的分类）。数据集被划为3个子集：训练集，验证集和测试集。\n",
    "\n",
    "- 定义**MNIST**数据的相关信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILES_PATTERN = 'data_csv/mnist_train.csv'\n",
    "VAL_DATA_FILES_PATTERN = 'data_csv/mnist_val.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'data_csv/mnist_test.csv'\n",
    "\n",
    "MULTI_THREADING = True\n",
    "RESUME_TRAINING = False\n",
    "\n",
    "NUM_CLASS = 10\n",
    "IMG_SHAPE = [28,28]\n",
    "\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_FLAT = 784\n",
    "NUM_CHANNEL = 1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_TRAIN = 55000\n",
    "NUM_VAL = 5000\n",
    "NUM_TEST = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data (10000, 784)\n",
      "test_label (10000,)\n",
      "val_data (5000, 784)\n",
      "val_label (5000,)\n",
      "train_data (55000, 784)\n",
      "train_label (55000,)\n"
     ]
    }
   ],
   "source": [
    "# train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN)\n",
    "# train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN, header=None, names=HEADER )\n",
    "train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN, header=None)\n",
    "test_data = pd.read_csv(TEST_DATA_FILES_PATTERN, header=None)\n",
    "val_data = pd.read_csv(VAL_DATA_FILES_PATTERN, header=None)\n",
    "\n",
    "train_values = train_data.values\n",
    "train_data = train_values[:,1:]/255.0\n",
    "train_label = train_values[:,0:1].squeeze()\n",
    "\n",
    "val_values = val_data.values\n",
    "val_data = val_values[:,1:]/255.0\n",
    "val_label = val_values[:,0:1].squeeze()\n",
    "\n",
    "test_values = test_data.values\n",
    "test_data = test_values[:,1:]/255.0\n",
    "test_label = test_values[:,0:1].squeeze()\n",
    "\n",
    "print('test_data',np.shape(test_data))\n",
    "print('test_label',np.shape(test_label))\n",
    "\n",
    "print('val_data',np.shape(val_data))\n",
    "print('val_label',np.shape(val_label))\n",
    "\n",
    "print('train_data',np.shape(train_data))\n",
    "print('train_label',np.shape(train_label))\n",
    "\n",
    "# train_data.head(10)\n",
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试试自己写一个estimator\n",
    "\n",
    "- 基于MNIST数据集，运用TensorFlow中的 **tf.estimator** 中的 **tf.estimator.Estimator** 搭建一个简单的卷积神经网络，实现模型的训练，验证和测试\n",
    "\n",
    "- [官网API](https://tensorflow.google.cn/api_docs/python/tf/estimator/Estimator)\n",
    "\n",
    "- 看看有哪些参数\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    model_fn,\n",
    "    model_dir=None,\n",
    "    config=None,\n",
    "    params=None,\n",
    "    warm_start_from=None\n",
    ")\n",
    "```\n",
    "- 本例中，主要用了 **tf.estimator.Estimator** 中的 `model_fn`,`model_dir`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先简单看看数据流\n",
    "\n",
    "下面的图表直接显示了本次MNIST例子的数据流向，共有**2个卷积层**，每一层卷积之后采用最大池化进行下采样（图中并未画出），最后接**2个全连接层**，实现对MNIST数据集的分类\n",
    "\n",
    "![Flowchart](images/02_network_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看看input_fn之创建输入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate tf.data.TextLineDataset() using make_one_shot_iterator()\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28,1])\n",
    "    features = tf.image.flip_left_right(features)\n",
    "#     print('features_aug',features_aug)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "#     label = tf.one_hot(label,num_class)\n",
    "    return features,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                 skip_header_lines=1, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=128):\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "        \n",
    "    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(files_name_pattern))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TextLineDataset(filenames=file_names).skip(1)\n",
    "#     dataset = tf.data.TextLineDataset(filenames).skip(1)\n",
    "    print(\"DATASET\",dataset)\n",
    "\n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(decode_line)\n",
    "    print(\"DATASET_1\",dataset)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    print(\"DATASET_2\",dataset)\n",
    "    dataset = dataset.batch(32)\n",
    "    print(\"DATASET_3\",dataset)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    print(\"DATASET_4\",dataset)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    \n",
    "    features = {'images':features}\n",
    "    \n",
    "    return features,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_train.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: train\n",
      "Thread Count: 4\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "DATASET <SkipDataset shapes: (), types: tf.string>\n",
      "features_aug Tensor(\"flip_left_right/ReverseV2:0\", shape=(28, 28, 1), dtype=float32)\n",
      "DATASET_1 <MapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <RepeatDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "Features in CSV: ['images']\n",
      "Target in CSV: Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "features, target = csv_input_fn(files_name_pattern=TRAIN_DATA_FILES_PATTERN)\n",
    "print(\"Features in CSV: {}\".format(list(features.keys())))\n",
    "print(\"Target in CSV: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x = tf.feature_column.numeric_column('images', shape=IMG_SHAPE)\n",
    "\n",
    "feature_columns = [feature_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重点在这里——model_fn\n",
    "\n",
    "\n",
    "#### model_fn: Model function. Follows the signature:\n",
    "\n",
    "* Args:\n",
    "  * `features`: This is the first item returned from the `input_fn` passed to `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor` or `dict` of same.\n",
    "  * `labels`: This is the second item returned from the `input_fn` passed to `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor` or `dict` of same (for multi-head models).If mode is @{tf.estimator.ModeKeys.PREDICT}, `labels=None` will be passed. If the `model_fn`'s signature does not accept `mode`, the `model_fn` must still be able to handle `labels=None`.\n",
    "  * `mode`: Optional. Specifies if this training, evaluation or prediction. See `tf.estimator.ModeKeys`.\n",
    "  * `params`: Optional `dict` of hyperparameters.  Will receive what is passed to Estimator in `params` parameter. This allows to configure Estimators from hyper parameter tuning.\n",
    "  * `config`: Optional `estimator.RunConfig` object. Will receive what is passed to Estimator as its `config` parameter, or a default value. Allows setting up things in your `model_fn` based on configuration such as `num_ps_replicas`, or `model_dir`.\n",
    "* Returns:\n",
    "    `tf.estimator.EstimatorSpec`\n",
    "    \n",
    "#### 注意model_fn返回的tf.estimator.EstimatorSpec\n",
    "<img style=\"float: left;\" src=\"images/0_TF_HELLO.png\" width=\"60%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义我们自己的model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Args:\n",
    "    #\n",
    "    # features: This is the x-arg from the input_fn.\n",
    "    # labels:   This is the y-arg from the input_fn,\n",
    "    #           see e.g. train_input_fn for these two.\n",
    "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
    "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
    "    \n",
    "    # Reference to the tensor named \"x\" in the input-function.\n",
    "#     x = features[\"images\"]\n",
    "    x = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # The convolutional layers expect 4-rank tensors\n",
    "    # but x is a 2-rank tensor, so reshape it.\n",
    "    net = tf.reshape(x, [-1, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL])    \n",
    "\n",
    "    # First convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                           filters=16, kernel_size=5,\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
    "\n",
    "    # Second convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
    "                           filters=36, kernel_size=5,\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    \n",
    "\n",
    "    # Flatten to a 2-rank tensor.\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    # Eventually this should be replaced with:\n",
    "    # net = tf.layers.flatten(net)\n",
    "\n",
    "    # First fully-connected / dense layer.\n",
    "    # This uses the ReLU activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
    "                          units=128, activation=tf.nn.relu)    \n",
    "\n",
    "    # Second fully-connected / dense layer.\n",
    "    # This is the last layer so it does not use an activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc2',\n",
    "                          units=10)\n",
    "\n",
    "    # Logits output of the neural network.\n",
    "    logits = net\n",
    "\n",
    "    # Softmax output of the neural network.\n",
    "    y_pred = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Classification output of the neural network.\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If the estimator is supposed to be in prediction-mode\n",
    "        # then use the predicted class-number that is output by\n",
    "        # the neural network. Optimization etc. is not needed.\n",
    "        spec = tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=y_pred_cls)\n",
    "    else:\n",
    "        # Otherwise the estimator is supposed to be in either\n",
    "        # training or evaluation-mode. Note that the loss-function\n",
    "        # is also required in Evaluation mode.\n",
    "        \n",
    "        # Define the loss-function to be optimized, by first\n",
    "        # calculating the cross-entropy between the output of\n",
    "        # the neural network and the true labels for the input data.\n",
    "        # This gives the cross-entropy for each image in the batch.\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                       logits=logits)\n",
    "\n",
    "        # Reduce the cross-entropy batch-tensor to a single number\n",
    "        # which can be used in optimization of the neural network.\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        # Define the optimizer for improving the neural network.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Get the TensorFlow op for doing a single optimization step.\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Define the evaluation metrics,\n",
    "        # in this case the classification accuracy.\n",
    "        metrics = \\\n",
    "        {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
    "        }\n",
    "\n",
    "        # Wrap all of this in an EstimatorSpec.\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=metrics)\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自建的estimator在这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": 1e-4,\n",
    "         'feature_columns': feature_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './cnn_classifer_dataset/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024A286E66A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                               params=params,\n",
    "                               model_dir=\"./cnn_classifer_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练训练看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_train.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: train\n",
      "Thread Count: 4\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "DATASET <SkipDataset shapes: (), types: tf.string>\n",
      "DATASET_1 <MapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <RepeatDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./cnn_classifer_dataset/model.ckpt.\n",
      "INFO:tensorflow:loss = 40.126976, step = 1\n",
      "INFO:tensorflow:global_step/sec: 11.057\n",
      "INFO:tensorflow:loss = 1.1582, step = 101 (9.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9123\n",
      "INFO:tensorflow:loss = 0.778288, step = 201 (7.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.889\n",
      "INFO:tensorflow:loss = 1.0873605, step = 301 (7.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1931\n",
      "INFO:tensorflow:loss = 0.07414566, step = 401 (7.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2251\n",
      "INFO:tensorflow:loss = 0.32521993, step = 501 (7.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7967\n",
      "INFO:tensorflow:loss = 0.2568686, step = 601 (7.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4253\n",
      "INFO:tensorflow:loss = 0.54189134, step = 701 (8.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5796\n",
      "INFO:tensorflow:loss = 0.15989298, step = 801 (7.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7096\n",
      "INFO:tensorflow:loss = 0.90422636, step = 901 (7.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8366\n",
      "INFO:tensorflow:loss = 0.20136827, step = 1001 (7.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5184\n",
      "INFO:tensorflow:loss = 0.53505665, step = 1101 (7.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8457\n",
      "INFO:tensorflow:loss = 0.22107196, step = 1201 (7.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0342\n",
      "INFO:tensorflow:loss = 0.31935138, step = 1301 (7.672 sec)\n"
     ]
    }
   ],
   "source": [
    "input_fn = lambda: csv_input_fn(\\\n",
    "                                files_name_pattern= TRAIN_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.TRAIN)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证一下瞅瞅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_val.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: eval\n",
      "Thread Count: 4\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "DATASET <SkipDataset shapes: (), types: tf.string>\n",
      "DATASET_1 <MapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <RepeatDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-23-12:36:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/simple_cnn/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-23-12:36:29\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.96875, global_step = 4000, loss = 0.1153331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96875, 'global_step': 4000, 'loss': 0.1153331}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = lambda: csv_input_fn(files_name_pattern= VAL_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "model.evaluate(input_fn,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_test.csv\n",
      "Batch size: 10\n",
      "Epoch Count: None\n",
      "Mode: infer\n",
      "Thread Count: 4\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "DATASET <SkipDataset shapes: (), types: tf.string>\n",
      "DATASET_1 <MapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <BatchDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <RepeatDataset shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/simple_cnn/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "PREDICTIONS [6, 1, 2, 7, 0, 8, 0, 3, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "input_fn = lambda: csv_input_fn(\\\n",
    "                                files_name_pattern= TEST_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.PREDICT,batch_size=10)\n",
    "\n",
    "predictions = list(itertools.islice(model.predict(input_fn=input_fn),10))\n",
    "print('PREDICTIONS',predictions)\n",
    "# print(\"\")\n",
    "# print(\"* Predicted Classes: {}\".format(list(map(lambda item: item[\"classes\"][0]\n",
    "#     ,predictions))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
