{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: From Estimators to Keras\n",
    "* Building a custom TensorFlow estimator (as a reference)\n",
    "    1. Use **Census** classification dataset\n",
    "    2. Create **feature columns** from the estimator\n",
    "    3. Implement a **tf.data input_fn**\n",
    "    4. Create a custom estimator using **tf.keras.layers**\n",
    "    5. **Train** and **evaluate** the model\n",
    "* Building a Functional Keras model and using tf.data APIs\n",
    "    1. Modify the **input_fn** to process categorical features\n",
    "    2. Build a Functional Keras Model\n",
    "    3. Use the input_fn to fit the Keras model\n",
    "    4. Configure **epochs** and **validation**\n",
    "    5. Configure **callbacks** for **early stopping** and **checkpoints**\n",
    "* Save and Load Keras model\n",
    "* Export Keras model to saved_model\n",
    "* Converting Keras model to estimator\n",
    "* Concluding Remarks\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/tf_train_eval_export/Tutorial%20-%20TensorFlow%20from%20Estimators%20to%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print(\"TensorFlow : {}\".format(tf.__version__))\n",
    "\n",
    "SEED = 19831060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='data'\n",
    "!mkdir $DATA_DIR\n",
    "!gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.data.csv $DATA_DIR\n",
    "!gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.test.csv $DATA_DIR\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'adult.data.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR, 'adult.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_SIZE = 32561\n",
    "EVAL_DATA_SIZE = 16278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                       [0], [0], [0], [''], ['']]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "CATEGORICAL_FEATURE_NAMES = ['gender', 'race', 'education', 'marital_status', 'relationship', \n",
    "                             'workclass', 'occupation', 'native_country']\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "TARGET_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'\n",
    "NUM_CLASSES = len(TARGET_LABELS)\n",
    "\n",
    "def get_categorical_features_vocabolary():\n",
    "    data = pd.read_csv(TRAIN_DATA_FILE, names=HEADER)\n",
    "    return {\n",
    "        column: list(data[column].unique()) \n",
    "        for column in data.columns if column in CATEGORICAL_FEATURE_NAMES\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vocabolary = get_categorical_features_vocabolary()\n",
    "print(feature_vocabolary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a TensorFlow Custom Estimator\n",
    "\n",
    "1. Creating feature columns\n",
    "2. Creating model_fn\n",
    "3. Create estimator using the model_fn\n",
    "4. Define data input_fn\n",
    "5. Define Train and evaluate experiment\n",
    "6. Run experiment with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "    \n",
    "    feature_columns = []\n",
    "    \n",
    "    for column in NUMERIC_FEATURE_NAMES:\n",
    "        feature_column = tf.feature_column.numeric_column(column)\n",
    "        feature_columns.append(feature_column)\n",
    "        \n",
    "    for column in CATEGORICAL_FEATURE_NAMES:\n",
    "        vocabolary = feature_vocabolary[column]\n",
    "        embed_size = int(math.sqrt(len(vocabolary)))\n",
    "        feature_column = tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list(column, vocabolary), \n",
    "            embed_size)\n",
    "        feature_columns.append(feature_column)\n",
    "        \n",
    "    return feature_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create model_fn\n",
    "1. Use feature columns to create input_layer\n",
    "2. Use tf.keras.layers to define the model architecutre and output\n",
    "3. Use binary_classification_head for create EstimatorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    # model body\n",
    "    def _inference(features, mode, params):\n",
    "        \n",
    "        feature_columns = create_feature_columns()\n",
    "        input_layer = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
    "        dense_inputs = input_layer\n",
    "        for i in range(len(params.hidden_units)):\n",
    "            dense = tf.keras.layers.Dense(params.hidden_units[i], activation='relu')(dense_inputs)\n",
    "            dense_dropout = tf.keras.layers.Dropout(params.dropout_prob)(dense, training=is_training)\n",
    "            dense_inputs = dense_dropout\n",
    "        fully_connected = dense_inputs  \n",
    "        logits = tf.keras.layers.Dense(units=1, name='logits', activation=None)(fully_connected)\n",
    "        return logits\n",
    "    \n",
    "    # model head\n",
    "    head = tf.contrib.estimator.binary_classification_head(\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        weight_column=WEIGHT_COLUMN_NAME\n",
    "    )\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        mode=mode,\n",
    "        logits=_inference(features, mode, params),\n",
    "        labels=labels,\n",
    "        optimizer=tf.train.AdamOptimizer(params.learning_rate)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "    \n",
    "    feature_columns = create_feature_columns()\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn,\n",
    "        params=params,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(file_pattern, batch_size, num_epochs, \n",
    "                  mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            batch_size=batch_size,\n",
    "            column_names=HEADER,\n",
    "            column_defaults=HEADER_DEFAULTS,\n",
    "            label_name=TARGET_NAME,\n",
    "            field_delim=',',\n",
    "            use_quote_delim=True,\n",
    "            header=False,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle= (mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experiment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_experiment(params, run_config):\n",
    "    \n",
    "    # TrainSpec ####################################\n",
    "    train_input_fn = make_input_fn(\n",
    "        TRAIN_DATA_FILE,\n",
    "        batch_size=params.batch_size,\n",
    "        num_epochs=None,\n",
    "        mode=tf.estimator.ModeKeys.TRAIN\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps=params.traning_steps\n",
    "    )\n",
    "    ###############################################    \n",
    "    \n",
    "    # EvalSpec ####################################\n",
    "    eval_input_fn = make_input_fn(\n",
    "        EVAL_DATA_FILE,\n",
    "        num_epochs=1,\n",
    "        batch_size=params.batch_size,\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        name=datetime.utcnow().strftime(\"%H%M%S\"),\n",
    "        input_fn = eval_input_fn,\n",
    "        steps=None,\n",
    "        start_delay_secs=0,\n",
    "        throttle_secs=params.eval_throttle_secs\n",
    "    )\n",
    "    ###############################################\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    if tf.gfile.Exists(run_config.model_dir):\n",
    "        print(\"Removing previous artefacts...\")\n",
    "        tf.gfile.DeleteRecursively(run_config.model_dir)\n",
    "            \n",
    "    print('')\n",
    "    estimator = create_estimator(params, run_config)\n",
    "    print('')\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Experiment with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_LOCATION = 'models/census'\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "\n",
    "params  = tf.contrib.training.HParams(\n",
    "    batch_size=200,\n",
    "    traning_steps=1000,\n",
    "    hidden_units=[100, 70, 50],\n",
    "    learning_rate=0.01,\n",
    "    dropout_prob=0.2,\n",
    "    eval_throttle_secs=0,\n",
    ")\n",
    "\n",
    "strategy = None\n",
    "num_gpus = len([device_name for device_name in tf.contrib.eager.list_devices()\n",
    "                if '/device:GPU' in device_name])\n",
    "\n",
    "if num_gpus > 1:\n",
    "    strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "    params.batch_size = int(math.ceil(params.batch_size / num_gpus))\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=SEED,\n",
    "    save_checkpoints_steps=200,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir,\n",
    "    train_distribute=strategy\n",
    ")\n",
    "\n",
    "train_and_evaluate_experiment(params, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Keras Model\n",
    "1. Implement a data input_fn process the data for the Keras model\n",
    "2. Create the Keras model\n",
    "3. Create the callbacks\n",
    "4. Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data input_fn\n",
    "A typical way of feed data into Keras is to convert it to a **numpy array** and pass it to the **model.fit()** function of the model. However, in other (probably more parctical) cases, all the data may not fit into the memory of your worker. Thus, you woud need to either create a reader that reads your data chuck by chuck, and pass it to **model.fit_generator()**, or to use the **tf.data.Dataset** APIs, which are much easier to use.\n",
    "\n",
    "In the input_fn, \n",
    "1. Create a CSV dataset (similar to the one used with the TensorFlow Custom Estimator)\n",
    "2. Create lookups for categorical features vocabolary to numerical index\n",
    "3. Process the dataset features to:\n",
    "    * extrat the instance weight column\n",
    "    * convert the categorical features to numerical index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keras_input_fn(file_pattern, batch_size, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \n",
    "    mapping_tables = {}\n",
    "        \n",
    "    mapping_tables[TARGET_NAME] = tf.contrib.lookup.index_table_from_tensor(\n",
    "        mapping=tf.constant(TARGET_LABELS))\n",
    "\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        mapping_tables[feature_name] = tf.contrib.lookup.index_table_from_tensor(\n",
    "            mapping=tf.constant(feature_vocabolary[feature_name]))\n",
    "    try:\n",
    "        tf.tables_initializer().run(session=tf.keras.backend.get_session()) \n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    def _process_features(features, target):\n",
    "        \n",
    "        weight = features.pop(WEIGHT_COLUMN_NAME)\n",
    "        target = mapping_tables[TARGET_NAME].lookup(target)\n",
    "        for feature in CATEGORICAL_FEATURE_NAMES:\n",
    "            features[feature] = mapping_tables[feature].lookup(features[feature])\n",
    "        return features, target, weight\n",
    "                        \n",
    "    def _input_fn():\n",
    "        \n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            batch_size=batch_size,\n",
    "            column_names=HEADER,\n",
    "            column_defaults=HEADER_DEFAULTS,\n",
    "            label_name=TARGET_NAME,\n",
    "            field_delim=',',\n",
    "            use_quote_delim=True,\n",
    "            header=False,\n",
    "            shuffle= (mode==tf.estimator.ModeKeys.TRAIN)\n",
    "        ).map(_process_features)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the keras model\n",
    "1. Create the model architecture: because Keras models do not suppurt feature columns (yet), we need to create:\n",
    "    * One input for each feature\n",
    "    * Embedding layer for each categorical feature\n",
    "    * Sigmoid output\n",
    "2. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \n",
    "    inputs = []\n",
    "    to_concat = []\n",
    "\n",
    "    for column in HEADER:\n",
    "        if column not in [WEIGHT_COLUMN_NAME, TARGET_NAME]:\n",
    "            if column in NUMERIC_FEATURE_NAMES:\n",
    "                numeric_input = tf.keras.layers.Input(shape=(1, ), name=column, dtype='float32')\n",
    "                inputs.append(numeric_input)\n",
    "                to_concat.append(numeric_input)\n",
    "            else:\n",
    "                categorical_input = tf.keras.layers.Input(shape=(1, ), name=column, dtype='int32')\n",
    "                inputs.append(categorical_input)\n",
    "                vocabulary_size = len(feature_vocabolary[column])\n",
    "                embed_size = int(math.sqrt(vocabulary_size))\n",
    "                embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size, \n",
    "                                                      output_dim=embed_size)(categorical_input)\n",
    "                reshape = tf.keras.layers.Reshape(target_shape=(embed_size, ))(embedding)\n",
    "                to_concat.append(reshape)\n",
    "                    \n",
    "    input_layer = tf.keras.layers.Concatenate(-1)(to_concat)    \n",
    "    dense_inputs = input_layer\n",
    "    for i in range(len(params.hidden_units)):\n",
    "        dense = tf.keras.layers.Dense(params.hidden_units[i], activation='relu')(dense_inputs)\n",
    "        dense_dropout = tf.keras.layers.Dropout(params.dropout_prob)(dense)#, training=is_training)\n",
    "        dense_inputs = dense_dropout\n",
    "    fully_connected = dense_inputs  \n",
    "    logits = tf.keras.layers.Dense(units=1, name='logits', activation=None)(fully_connected)\n",
    "    \n",
    "    sigmoid = tf.keras.layers.Activation(activation='sigmoid', name='probability')(logits)\n",
    "\n",
    "    # keras model\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=sigmoid)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $model_dir/checkpoints\n",
    "!ls $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define callbacks \n",
    "1. Early stopping callback\n",
    "2. Checkpoints callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(model_dir,'checkpoints', 'model-{epoch:02d}.h5'), \n",
    "        monitor='val_loss', \n",
    "        period=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.training_utils import multi_gpu_model\n",
    "model = create_model(params)\n",
    "# model = multi_gpu_model(model, gpus=4) # This is to train the model with multiple GPUs\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run experiment\n",
    "When using out-of-memory dataset, that is, reading data chuck by chuck from file(s) and feeding it to the model (using the tf.data.Dataset APIs), you usually do not know the size of the dataset. Thus, beside the number of epochs required to train the model for, you need to specify how many step is considered as an epoch. \n",
    "\n",
    "This is not required when use an in-memory (numpy array) dataset, since the size of the dataset is know to the model, hence how many steps here are in the epoch.\n",
    "\n",
    "In our experiment, we know the size of our dataset, thus we compute the **steps_per_epoch** as: **training data size /batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_keras_input_fn(\n",
    "    TRAIN_DATA_FILE,\n",
    "    batch_size=params.batch_size,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN\n",
    ")()\n",
    "\n",
    "\n",
    "valid_data = make_keras_input_fn(\n",
    "    EVAL_DATA_FILE,\n",
    "    batch_size=params.batch_size,\n",
    "    mode=tf.estimator.ModeKeys.EVAL\n",
    ")()\n",
    "\n",
    "steps_per_epoch = int(math.ceil(TRAIN_DATA_SIZE/float(params.batch_size)))\n",
    "model.fit(\n",
    "    train_data, \n",
    "    epochs=5, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_data,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $model_dir/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Keras Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_dir = os.path.join(model_dir, 'keras_classifier.h5')\n",
    "model.save(keras_model_dir)\n",
    "print(\"Keras model saved to: {}\".format(keras_model_dir))\n",
    "model = tf.keras.models.load_model(keras_model_dir)\n",
    "print(\"Keras model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = make_keras_input_fn(\n",
    "        EVAL_DATA_FILE,\n",
    "        batch_size=5,\n",
    "        mode=tf.estimator.ModeKeys.EVAL\n",
    "    )()\n",
    "\n",
    "predictions = map(\n",
    "    lambda probability: TARGET_LABELS[0] if probability <0.5 else TARGET_LABELS[1], \n",
    "    model.predict(predict_data, steps=1)\n",
    ")\n",
    "\n",
    "print(list(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Keras Model as saved_model for tf.Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MODEL_DIR'] = model_dir\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "from tensorflow.contrib.saved_model.python.saved_model import keras_saved_model\n",
    "keras_saved_model.save_keras_model(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=${MODEL_DIR}/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Estimator for Distributed Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
