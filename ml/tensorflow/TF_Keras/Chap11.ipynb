{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0xuMOWdmCLFx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-22 11:42:20.722006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-10-22 11:42:20.832382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-10-22 11:42:20.832428: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-10-22 11:42:20.832464: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ocsbesrhlrepo01.amr.corp.intel.com): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "'''\n",
        "初始化运算图，它包含了上节提到的各个运算单元，它将为W,x,b，h构造运算部件，并将它们连接\n",
        "起来\n",
        "'''\n",
        "graph = tf.Graph()\n",
        "#一次tensorflow代码的运行都要初始化一个session\n",
        "session = tf.InteractiveSession(graph=graph)\n",
        "'''\n",
        "我们定义三种变量，一种叫placeholder，它对应输入变量，也就是上节计算图所示的圆圈部分,\n",
        "他们的值在计算开始进行时才确定，这里对应x;\n",
        "一种叫Variables,他们的值一开始就初始化，在后续运算中可以进行更改,这里对应W,b；\n",
        "一种叫immutable tensor，这里对应h，它的值不允许我们直接修改\n",
        "'''\n",
        "x = tf.placeholder(shape=[1, 10], dtype=tf.float32, name='x')\n",
        "#将W的各个分量初始化到[-0.1, 0.1]之间\n",
        "W = tf.Variable(tf.random_uniform(shape=[10,5], minval=-0.1, \n",
        "                                  maxval=0.1, dtype=tf.float32), name='W')\n",
        "#把b的各个分量初始化为0\n",
        "b = tf.Variable(tf.zeros(shape=[5], dtype=tf.float32), name='b')\n",
        "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TTm8Qo2E7r_0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-22 11:42:24.178539: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
          ]
        }
      ],
      "source": [
        " #使用run为各个变量分配内存并执行初始化操作\n",
        "tf.global_variables_initializer().run()\n",
        "h_eval = session.run(h, feed_dict={x: np.random.rand(1,10)})\n",
        "#结束时一定要关闭session\n",
        "session.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mvIK-HCE7_4i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_529383/3764744152.py:10: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /home/taosy/.local/lib/python3.9/site-packages/tensorflow/python/training/input.py:262: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /home/taosy/.local/lib/python3.9/site-packages/tensorflow/python/training/input.py:184: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /home/taosy/.local/lib/python3.9/site-packages/tensorflow/python/training/input.py:192: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /home/taosy/.local/lib/python3.9/site-packages/tensorflow/python/training/input.py:191: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Failed to find file: test1.txt",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m filenames:\n\u001b[1;32m     15\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mExists(f):\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFailed to find file: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m f)\n\u001b[1;32m     17\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m found.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mf)\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find file: test1.txt"
          ]
        }
      ],
      "source": [
        "'''\n",
        "先启动运算图和session\n",
        "'''\n",
        "graph = tf.Graph()\n",
        "session = tf.InteractiveSession(graph = graph)\n",
        "\n",
        "#将要读入文件名存储在一个队列中\n",
        "filenames = ['./chap11/test%d.txt' %i for i in range(1,4)]\n",
        "#下面建立一个输入管道\n",
        "filename_queue = tf.train.string_input_producer(filenames, capacity = 3, \n",
        "                                                shuffle = True, \n",
        "                                                name = 'string_input_procucer')\n",
        "\n",
        "for f in filenames:\n",
        "  if not tf.gfile.Exists(f):\n",
        "    raise ValueError('Failed to find file: ' + f)\n",
        "  else:\n",
        "    print('File %s found.' %f)\n",
        "\n",
        "#构建reader将数据全部读入,tensorflow提供多种reader让我们读入不同格式数据\n",
        "reader = tf.TextLineReader()\n",
        "'''\n",
        "调用reader.read读入数据，它一次读入一行,read返回数据结构(key, value),其中key对应读入数据的\n",
        "文件名,value对应读入的一行数据\n",
        "'''\n",
        "key, value = reader.read(filename_queue, name='text_read_op')\n",
        "\n",
        "'''\n",
        "使用decoder将读入数据解码成指定数据结构，这里我们把读入的一行数据分解成多个数据列,由于每行包含\n",
        "10个数字，因此对应10个数据列，因为一个文本包含5行数据，三个文本总共包含15行，因此一列数据包含\n",
        "15个数字\n",
        "'''\n",
        "record_defaults = [[-1.0],[-1.0],[-1.0],[-1.0],[-1.0],[-1.0],\n",
        "                   [-1.0],[-1.0],[-1.0],[-1.0],]\n",
        "col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 = tf.decode_csv(value, record_defaults = record_defaults)\n",
        "\n",
        "#把数据列合在一起形成二维向量\n",
        "features = tf.stack([col1,col2,col3,col4,col5,col6,col7,col8,col9,col10])\n",
        "'''\n",
        "在训练网络时，我们往往需要很多训练数据，当数据量庞大时，一次将数据全部读入内存是不现实的，\n",
        "因此我们需要开辟一片缓存，然后将数据分批读入，capacity表示缓存最多能读入几条数据，\n",
        "batch_size表示一次将相应条数据进行读取处理，\n",
        "min_after_dequeue表示缓存中至少要读入几条数据,num_threads表示使用几个线程进行操作\n",
        "'''\n",
        "x = tf.train.shuffle_batch([features], batch_size=3, capacity=5,\n",
        "                          name='data_batch', min_after_dequeue=1,\n",
        "                          num_threads = 1)\n",
        "#启动输入管道的运行流程\n",
        "'''\n",
        "由于数据读入和预处理是一种非常耗时的工作,tensorflow会创建多个线程同时对数据进行读取和处理，\n",
        "coord对应所有处理线程的管理器，start_queue_runners则启动所有处理线程\n",
        "'''\n",
        "coord = tf.train.Coordinator()\n",
        "threads = tf.train.start_queue_runners(coord=coord, sess=session)\n",
        "\n",
        "W = tf.Variable(tf.random_uniform(shape=[10,5], minval=-0.1,\n",
        "                                 maxval=0.1, dtype=tf.float32), name = 'W')\n",
        "b = tf.Variable(tf.zeros(shape=[5], dtype=tf.float32), name='b')\n",
        "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
        "\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "for step in range(5):\n",
        "  x_eval, h_eval = session.run([x, h])\n",
        "  print('====step %d ====' %step)\n",
        "  print('Evaluated data (x)')\n",
        "  print(x_eval)\n",
        "  print('Evaluated data (h)')\n",
        "  print(h_eval)\n",
        "  print('')\n",
        "  \n",
        "#终止数据管道线程\n",
        "coord.request_stop()\n",
        "coord.join(threads)\n",
        "session.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_ehozqtr8Lbl"
      },
      "outputs": [],
      "source": [
        "def  print_tensor(tensor):\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    v = sess.run(tensor)\n",
        "    print(v)  # will show you your variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "uyW4_Xg28Mu4",
        "outputId": "08efaed6-c83e-4ad8-b425-1dce6b1c834c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 2. 3. 4. 5.]\n"
          ]
        }
      ],
      "source": [
        "ref = tf.Variable(tf.constant([1,9,3,10,5], dtype=tf.float32), name='scatter_value')\n",
        "indices = [1,3]\n",
        "updates = tf.constant([2,4], dtype = tf.float32)\n",
        "tf_scatter_update = tf.scatter_update(ref, indices, updates,\n",
        "                                     use_locking=None,\n",
        "                                     name=None)\n",
        "\n",
        "print_tensor(tf_scatter_update)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "W-3vOIhD8S1b",
        "outputId": "9b9505b7-9520-47f9-9bf8-18d74d870980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]\n",
            " [2 2 2]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "下面代码要构造一个4*3的二维向量，同时指定把updates变量对应的一维向量安置到indices指定位置，\n",
        "也就是把[1,1,1]作为4*3二维向量的第1行，把[2,2,2]作为4*3向量的第3行，其他行自动初始化为0，\n",
        "'''\n",
        "indices=[[1], [3]]\n",
        "updates = tf.constant([[1,1,1], [2,2,2]])\n",
        "shape = [4,3]\n",
        "tf_scatter_nd_1 = tf.scatter_nd(indices, updates, shape, name=None)\n",
        "print_tensor(tf_scatter_nd_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "hLaet5u88V7l",
        "outputId": "ab494419-dbfd-4980-bfdc-5d02b6c9653a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0]\n",
            " [2 0 0]\n",
            " [0 0 0]\n",
            " [0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "构造一个4*3二维向量，然后把数值2插入到第1行第0列，\n",
        "把数值1插入到第3行第2列，其他值初始化为0\n",
        "'''\n",
        "indices = [[1,0], [3,2]]\n",
        "updates = tf.constant([2,1])\n",
        "shape = [4,3]\n",
        "tf_scatter_nd_2 = tf.scatter_nd(indices, updates, shape, name=None)\n",
        "print_tensor(tf_scatter_nd_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "UDDGDM5m8akV",
        "outputId": "fd4a91a9-bf3a-41b3-ef40-67860b22654a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2. 5.]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "下面代码把向量[1,2,3,4,5]中下标为1，4的分量提取出来，因此得到向量\n",
        "[2,5]\n",
        "'''\n",
        "params = tf.constant([1,2,3,4,5], dtype=tf.float32)\n",
        "indices = [1,4]\n",
        "tf_gather = tf.gather(params, indices, validate_indices = True, name = None)\n",
        "print_tensor(tf_gather)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "7CXNMpg38cP9",
        "outputId": "62035e29-df02-444b-a0c3-ba740e01f438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0]\n",
            " [2 2 2]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "把二维向量中指定行提取出来\n",
        "'''\n",
        "params = tf.constant([[0,0,0], [1,1,1], [2,2,2],[3,3,3]])\n",
        "indices = [[0], [2]]\n",
        "tf_gather_nd = tf.gather_nd(params, indices, name=None)\n",
        "print_tensor(tf_gather_nd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "HgJspxPg8kQ-",
        "outputId": "4445020c-3553-4be9-b7d2-51e68203a436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[ 7.5]\n",
            "   [ 7.5]\n",
            "   [ 7.5]]\n",
            "\n",
            "  [[13.5]\n",
            "   [13.5]\n",
            "   [13.5]]\n",
            "\n",
            "  [[19.5]\n",
            "   [19.5]\n",
            "   [19.5]]]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "构造一个4*4二维矩阵模拟图片\n",
        "'''\n",
        "x = tf.constant([[\n",
        "    [[1], [2], [3], [4]],\n",
        "    [[4], [3], [2], [1]],\n",
        "    [[5], [6], [7], [8]],\n",
        "    [[8], [7], [6], [5]]\n",
        "]], dtype=tf.float32)\n",
        "\n",
        "#定义用于做卷积操作的小矩阵\n",
        "x_filter = tf.constant([\n",
        "    [[[0.5]], [[1]]],\n",
        "    [[[0.5]], [[1]]]\n",
        "], dtype = tf.float32)\n",
        "\n",
        "'''\n",
        "把大矩阵切割成2*2小矩阵，然后与上面定义矩阵做乘机，\n",
        "然后每次向右或向下平移一个单位后再做对应小矩阵的乘机运算\n",
        "'''\n",
        "#定义一次平移距离,四个分量分别为batch_stride,height_stride,width_stride,channles_stride]\n",
        "x_strides = [1,1,1,1]\n",
        "x_padding = 'VALID'\n",
        "x_conv = tf.nn.conv2d(input = x, filter = x_filter,\n",
        "                     strides = x_strides, padding = x_padding)\n",
        "print_tensor(x_conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "Hn2bTy7U8nqw",
        "outputId": "89df979f-f8fc-4c29-f4e5-d9d5e6842376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[4.]\n",
            "   [4.]]\n",
            "\n",
            "  [[8.]\n",
            "   [8.]]]]\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant([[\n",
        "    [[1], [2], [3], [4]],\n",
        "    [[4], [3], [2], [1]],\n",
        "    [[5], [6], [7], [8]],\n",
        "    [[8], [7], [5], [6]]\n",
        "]], dtype = tf.float32)\n",
        "\n",
        "'''\n",
        "把矩阵分割成2*2小矩阵\n",
        "'''\n",
        "x_ksize = [1, 2, 2 ,1]\n",
        "#做max pooling 时每次沿水平和竖直方向挪动2个单位\n",
        "x_stride = [1, 2, 2, 1]\n",
        "x_padding = 'VALID'\n",
        "\n",
        "x_pool = tf.nn.max_pool(value = x, ksize = x_ksize,\n",
        "                       strides = x_stride, padding = x_padding)\n",
        "print_tensor(x_pool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "jSLMa-dk1z4u",
        "outputId": "867ed2c6-eb04-4f4f-c037-75728fec2d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.0\n",
            "[0.31326166 0.04858733]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "和方差:\n",
        "MSE = (1^2 + 2^2 + 3^2 + 4^2) / 4 = 15\n",
        "'''\n",
        "x = tf.constant([[1,2], [3,4]], dtype = tf.float32)\n",
        "mse = tf.nn.l2_loss(x)\n",
        "print_tensor(mse)\n",
        "\n",
        "'''\n",
        "cross entropy \n",
        "H = -y*log(y') - (1-y)*log(1-y')\n",
        "'''\n",
        "y = tf.constant([[1,0],[0,1]], dtype = tf.float32)\n",
        "y_hat = tf.constant([[3,2], [2,5]], dtype = tf.float32)\n",
        "H = tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_hat, labels = y)\n",
        "print_tensor(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17017
        },
        "colab_type": "code",
        "id": "y8aPoPmCB7fP",
        "outputId": "e2deee61-35a3-4566-e26b-1e1f2ef3455a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step:  0  x:  1.96  y:  3.8416002\n",
            "step:  1  x:  1.9208001  y:  3.689473\n",
            "step:  2  x:  1.8823841  y:  3.5433698\n",
            "step:  3  x:  1.8447363  y:  3.403052\n",
            "step:  4  x:  1.8078417  y:  3.2682915\n",
            "step:  5  x:  1.7716849  y:  3.1388674\n",
            "step:  6  x:  1.7362512  y:  3.0145683\n",
            "step:  7  x:  1.7015262  y:  2.8951912\n",
            "step:  8  x:  1.6674956  y:  2.7805417\n",
            "step:  9  x:  1.6341457  y:  2.6704323\n",
            "step:  10  x:  1.6014628  y:  2.5646832\n",
            "step:  11  x:  1.5694336  y:  2.4631217\n",
            "step:  12  x:  1.5380449  y:  2.3655822\n",
            "step:  13  x:  1.507284  y:  2.2719052\n",
            "step:  14  x:  1.4771384  y:  2.181938\n",
            "step:  15  x:  1.4475956  y:  2.095533\n",
            "step:  16  x:  1.4186437  y:  2.0125499\n",
            "step:  17  x:  1.3902708  y:  1.932853\n",
            "step:  18  x:  1.3624654  y:  1.8563119\n",
            "step:  19  x:  1.335216  y:  1.7828019\n",
            "step:  20  x:  1.3085117  y:  1.7122029\n",
            "step:  21  x:  1.2823415  y:  1.6443996\n",
            "step:  22  x:  1.2566947  y:  1.5792814\n",
            "step:  23  x:  1.2315608  y:  1.5167421\n",
            "step:  24  x:  1.2069296  y:  1.456679\n",
            "step:  25  x:  1.182791  y:  1.3989946\n",
            "step:  26  x:  1.1591352  y:  1.3435944\n",
            "step:  27  x:  1.1359525  y:  1.290388\n",
            "step:  28  x:  1.1132334  y:  1.2392887\n",
            "step:  29  x:  1.0909687  y:  1.1902127\n",
            "step:  30  x:  1.0691494  y:  1.1430804\n",
            "step:  31  x:  1.0477664  y:  1.0978146\n",
            "step:  32  x:  1.0268111  y:  1.0543411\n",
            "step:  33  x:  1.0062749  y:  1.0125892\n",
            "step:  34  x:  0.98614943  y:  0.9724907\n",
            "step:  35  x:  0.96642643  y:  0.93398005\n",
            "step:  36  x:  0.9470979  y:  0.8969944\n",
            "step:  37  x:  0.92815596  y:  0.8614735\n",
            "step:  38  x:  0.90959287  y:  0.8273592\n",
            "step:  39  x:  0.891401  y:  0.7945957\n",
            "step:  40  x:  0.87357295  y:  0.7631297\n",
            "step:  41  x:  0.8561015  y:  0.7329098\n",
            "step:  42  x:  0.8389795  y:  0.70388657\n",
            "step:  43  x:  0.8221999  y:  0.67601264\n",
            "step:  44  x:  0.8057559  y:  0.6492426\n",
            "step:  45  x:  0.7896408  y:  0.6235326\n",
            "step:  46  x:  0.773848  y:  0.5988407\n",
            "step:  47  x:  0.75837106  y:  0.57512665\n",
            "step:  48  x:  0.74320364  y:  0.55235165\n",
            "step:  49  x:  0.72833955  y:  0.5304785\n",
            "step:  50  x:  0.7137728  y:  0.5094716\n",
            "step:  51  x:  0.69949734  y:  0.48929653\n",
            "step:  52  x:  0.6855074  y:  0.46992043\n",
            "step:  53  x:  0.6717973  y:  0.4513116\n",
            "step:  54  x:  0.6583613  y:  0.4334396\n",
            "step:  55  x:  0.6451941  y:  0.41627544\n",
            "step:  56  x:  0.63229024  y:  0.39979094\n",
            "step:  57  x:  0.61964446  y:  0.38395926\n",
            "step:  58  x:  0.6072516  y:  0.36875448\n",
            "step:  59  x:  0.59510654  y:  0.3541518\n",
            "step:  60  x:  0.5832044  y:  0.34012735\n",
            "step:  61  x:  0.5715403  y:  0.3266583\n",
            "step:  62  x:  0.5601095  y:  0.31372264\n",
            "step:  63  x:  0.5489073  y:  0.3012992\n",
            "step:  64  x:  0.5379291  y:  0.28936774\n",
            "step:  65  x:  0.52717054  y:  0.27790877\n",
            "step:  66  x:  0.51662713  y:  0.2669036\n",
            "step:  67  x:  0.5062946  y:  0.25633422\n",
            "step:  68  x:  0.4961687  y:  0.24618338\n",
            "step:  69  x:  0.48624533  y:  0.23643452\n",
            "step:  70  x:  0.47652042  y:  0.2270717\n",
            "step:  71  x:  0.46699002  y:  0.21807969\n",
            "step:  72  x:  0.4576502  y:  0.20944372\n",
            "step:  73  x:  0.4484972  y:  0.20114975\n",
            "step:  74  x:  0.43952727  y:  0.19318423\n",
            "step:  75  x:  0.43073672  y:  0.18553412\n",
            "step:  76  x:  0.42212197  y:  0.17818695\n",
            "step:  77  x:  0.41367954  y:  0.17113076\n",
            "step:  78  x:  0.40540594  y:  0.16435398\n",
            "step:  79  x:  0.39729783  y:  0.15784557\n",
            "step:  80  x:  0.38935187  y:  0.15159488\n",
            "step:  81  x:  0.38156483  y:  0.14559172\n",
            "step:  82  x:  0.37393352  y:  0.13982628\n",
            "step:  83  x:  0.36645484  y:  0.13428915\n",
            "step:  84  x:  0.35912573  y:  0.1289713\n",
            "step:  85  x:  0.35194322  y:  0.12386403\n",
            "step:  86  x:  0.34490436  y:  0.11895902\n",
            "step:  87  x:  0.3380063  y:  0.11424825\n",
            "step:  88  x:  0.33124617  y:  0.10972402\n",
            "step:  89  x:  0.32462123  y:  0.10537894\n",
            "step:  90  x:  0.3181288  y:  0.10120593\n",
            "step:  91  x:  0.3117662  y:  0.097198166\n",
            "step:  92  x:  0.30553088  y:  0.093349114\n",
            "step:  93  x:  0.29942027  y:  0.08965249\n",
            "step:  94  x:  0.29343185  y:  0.08610225\n",
            "step:  95  x:  0.2875632  y:  0.08269259\n",
            "step:  96  x:  0.28181195  y:  0.079417974\n",
            "step:  97  x:  0.2761757  y:  0.076273024\n",
            "step:  98  x:  0.2706522  y:  0.07325262\n",
            "step:  99  x:  0.26523915  y:  0.07035181\n",
            "step:  100  x:  0.25993437  y:  0.06756587\n",
            "step:  101  x:  0.25473568  y:  0.064890265\n",
            "step:  102  x:  0.24964097  y:  0.062320616\n",
            "step:  103  x:  0.24464816  y:  0.059852723\n",
            "step:  104  x:  0.2397552  y:  0.057482556\n",
            "step:  105  x:  0.2349601  y:  0.055206247\n",
            "step:  106  x:  0.2302609  y:  0.05302008\n",
            "step:  107  x:  0.22565567  y:  0.050920483\n",
            "step:  108  x:  0.22114256  y:  0.04890403\n",
            "step:  109  x:  0.2167197  y:  0.04696743\n",
            "step:  110  x:  0.21238531  y:  0.04510752\n",
            "step:  111  x:  0.2081376  y:  0.043321263\n",
            "step:  112  x:  0.20397484  y:  0.041605737\n",
            "step:  113  x:  0.19989535  y:  0.039958153\n",
            "step:  114  x:  0.19589745  y:  0.03837581\n",
            "step:  115  x:  0.1919795  y:  0.036856126\n",
            "step:  116  x:  0.1881399  y:  0.03539662\n",
            "step:  117  x:  0.1843771  y:  0.033994917\n",
            "step:  118  x:  0.18068956  y:  0.032648716\n",
            "step:  119  x:  0.17707577  y:  0.031355828\n",
            "step:  120  x:  0.17353426  y:  0.030114138\n",
            "step:  121  x:  0.17006357  y:  0.028921617\n",
            "step:  122  x:  0.1666623  y:  0.027776323\n",
            "step:  123  x:  0.16332906  y:  0.026676383\n",
            "step:  124  x:  0.16006248  y:  0.025619997\n",
            "step:  125  x:  0.15686123  y:  0.024605446\n",
            "step:  126  x:  0.153724  y:  0.023631068\n",
            "step:  127  x:  0.15064952  y:  0.022695277\n",
            "step:  128  x:  0.14763653  y:  0.021796545\n",
            "step:  129  x:  0.14468381  y:  0.020933405\n",
            "step:  130  x:  0.14179014  y:  0.020104444\n",
            "step:  131  x:  0.13895434  y:  0.019308308\n",
            "step:  132  x:  0.13617526  y:  0.018543702\n",
            "step:  133  x:  0.13345176  y:  0.017809372\n",
            "step:  134  x:  0.13078272  y:  0.01710412\n",
            "step:  135  x:  0.12816706  y:  0.016426796\n",
            "step:  136  x:  0.12560372  y:  0.015776295\n",
            "step:  137  x:  0.123091646  y:  0.015151553\n",
            "step:  138  x:  0.12062981  y:  0.014551551\n",
            "step:  139  x:  0.118217215  y:  0.01397531\n",
            "step:  140  x:  0.11585287  y:  0.0134218875\n",
            "step:  141  x:  0.113535814  y:  0.012890381\n",
            "step:  142  x:  0.1112651  y:  0.012379923\n",
            "step:  143  x:  0.1090398  y:  0.0118896775\n",
            "step:  144  x:  0.106859006  y:  0.011418847\n",
            "step:  145  x:  0.10472183  y:  0.010966661\n",
            "step:  146  x:  0.10262739  y:  0.010532381\n",
            "step:  147  x:  0.10057484  y:  0.010115299\n",
            "step:  148  x:  0.09856334  y:  0.009714733\n",
            "step:  149  x:  0.096592076  y:  0.00933003\n",
            "step:  150  x:  0.09466024  y:  0.008960561\n",
            "step:  151  x:  0.09276703  y:  0.008605721\n",
            "step:  152  x:  0.09091169  y:  0.008264935\n",
            "step:  153  x:  0.089093454  y:  0.007937644\n",
            "step:  154  x:  0.08731159  y:  0.0076233135\n",
            "step:  155  x:  0.08556536  y:  0.0073214304\n",
            "step:  156  x:  0.08385405  y:  0.0070315017\n",
            "step:  157  x:  0.08217697  y:  0.006753054\n",
            "step:  158  x:  0.08053343  y:  0.0064856336\n",
            "step:  159  x:  0.07892276  y:  0.0062288027\n",
            "step:  160  x:  0.077344306  y:  0.0059821415\n",
            "step:  161  x:  0.075797416  y:  0.0057452484\n",
            "step:  162  x:  0.07428147  y:  0.0055177365\n",
            "step:  163  x:  0.07279584  y:  0.005299234\n",
            "step:  164  x:  0.07133992  y:  0.005089384\n",
            "step:  165  x:  0.06991312  y:  0.0048878444\n",
            "step:  166  x:  0.06851485  y:  0.0046942853\n",
            "step:  167  x:  0.06714456  y:  0.0045083915\n",
            "step:  168  x:  0.065801665  y:  0.0043298593\n",
            "step:  169  x:  0.06448563  y:  0.0041583967\n",
            "step:  170  x:  0.06319592  y:  0.0039937245\n",
            "step:  171  x:  0.061932  y:  0.0038355729\n",
            "step:  172  x:  0.06069336  y:  0.0036836842\n",
            "step:  173  x:  0.059479494  y:  0.0035378102\n",
            "step:  174  x:  0.058289904  y:  0.003397713\n",
            "step:  175  x:  0.057124104  y:  0.0032631634\n",
            "step:  176  x:  0.05598162  y:  0.003133942\n",
            "step:  177  x:  0.05486199  y:  0.0030098378\n",
            "step:  178  x:  0.05376475  y:  0.0028906483\n",
            "step:  179  x:  0.052689455  y:  0.0027761788\n",
            "step:  180  x:  0.051635668  y:  0.0026662422\n",
            "step:  181  x:  0.050602954  y:  0.0025606588\n",
            "step:  182  x:  0.049590893  y:  0.0024592567\n",
            "step:  183  x:  0.048599076  y:  0.0023618701\n",
            "step:  184  x:  0.047627095  y:  0.0022683402\n",
            "step:  185  x:  0.046674553  y:  0.002178514\n",
            "step:  186  x:  0.045741063  y:  0.0020922448\n",
            "step:  187  x:  0.044826243  y:  0.002009392\n",
            "step:  188  x:  0.04392972  y:  0.0019298202\n",
            "step:  189  x:  0.043051124  y:  0.0018533992\n",
            "step:  190  x:  0.0421901  y:  0.0017800046\n",
            "step:  191  x:  0.0413463  y:  0.0017095165\n",
            "step:  192  x:  0.040519375  y:  0.0016418198\n",
            "step:  193  x:  0.039708987  y:  0.0015768036\n",
            "step:  194  x:  0.038914807  y:  0.0015143622\n",
            "step:  195  x:  0.038136512  y:  0.0014543935\n",
            "step:  196  x:  0.03737378  y:  0.0013967996\n",
            "step:  197  x:  0.036626305  y:  0.0013414862\n",
            "step:  198  x:  0.03589378  y:  0.0012883634\n",
            "step:  199  x:  0.035175905  y:  0.0012373442\n",
            "step:  200  x:  0.034472387  y:  0.0011883455\n",
            "step:  201  x:  0.03378294  y:  0.001141287\n",
            "step:  202  x:  0.03310728  y:  0.001096092\n",
            "step:  203  x:  0.032445136  y:  0.0010526868\n",
            "step:  204  x:  0.031796236  y:  0.0010110006\n",
            "step:  205  x:  0.031160312  y:  0.000970965\n",
            "step:  206  x:  0.030537106  y:  0.00093251484\n",
            "step:  207  x:  0.029926363  y:  0.0008955872\n",
            "step:  208  x:  0.029327836  y:  0.000860122\n",
            "step:  209  x:  0.02874128  y:  0.0008260612\n",
            "step:  210  x:  0.028166454  y:  0.0007933492\n",
            "step:  211  x:  0.027603125  y:  0.0007619325\n",
            "step:  212  x:  0.027051063  y:  0.00073176\n",
            "step:  213  x:  0.026510041  y:  0.0007027823\n",
            "step:  214  x:  0.025979841  y:  0.00067495217\n",
            "step:  215  x:  0.025460245  y:  0.0006482241\n",
            "step:  216  x:  0.02495104  y:  0.00062255445\n",
            "step:  217  x:  0.02445202  y:  0.00059790123\n",
            "step:  218  x:  0.023962978  y:  0.00057422434\n",
            "step:  219  x:  0.023483718  y:  0.000551485\n",
            "step:  220  x:  0.023014044  y:  0.00052964623\n",
            "step:  221  x:  0.022553764  y:  0.00050867227\n",
            "step:  222  x:  0.02210269  y:  0.00048852887\n",
            "step:  223  x:  0.021660635  y:  0.0004691831\n",
            "step:  224  x:  0.021227423  y:  0.0004506035\n",
            "step:  225  x:  0.020802874  y:  0.00043275958\n",
            "step:  226  x:  0.020386817  y:  0.00041562231\n",
            "step:  227  x:  0.01997908  y:  0.00039916363\n",
            "step:  228  x:  0.019579498  y:  0.00038335673\n",
            "step:  229  x:  0.019187909  y:  0.00036817582\n",
            "step:  230  x:  0.01880415  y:  0.00035359606\n",
            "step:  231  x:  0.018428067  y:  0.00033959365\n",
            "step:  232  x:  0.018059505  y:  0.00032614573\n",
            "step:  233  x:  0.017698316  y:  0.00031323038\n",
            "step:  234  x:  0.01734435  y:  0.00030082648\n",
            "step:  235  x:  0.016997462  y:  0.00028891372\n",
            "step:  236  x:  0.016657513  y:  0.00027747272\n",
            "step:  237  x:  0.016324362  y:  0.0002664848\n",
            "step:  238  x:  0.015997875  y:  0.000255932\n",
            "step:  239  x:  0.015677918  y:  0.0002457971\n",
            "step:  240  x:  0.015364359  y:  0.00023606353\n",
            "step:  241  x:  0.015057072  y:  0.00022671542\n",
            "step:  242  x:  0.014755931  y:  0.0002177375\n",
            "step:  243  x:  0.014460812  y:  0.0002091151\n",
            "step:  244  x:  0.014171596  y:  0.00020083413\n",
            "step:  245  x:  0.0138881635  y:  0.00019288108\n",
            "step:  246  x:  0.0136104  y:  0.000185243\n",
            "step:  247  x:  0.013338192  y:  0.00017790738\n",
            "step:  248  x:  0.013071429  y:  0.00017086226\n",
            "step:  249  x:  0.01281  y:  0.00016409611\n",
            "step:  250  x:  0.0125538  y:  0.0001575979\n",
            "step:  251  x:  0.012302724  y:  0.00015135702\n",
            "step:  252  x:  0.012056669  y:  0.00014536327\n",
            "step:  253  x:  0.011815536  y:  0.00013960688\n",
            "step:  254  x:  0.011579225  y:  0.00013407845\n",
            "step:  255  x:  0.01134764  y:  0.00012876894\n",
            "step:  256  x:  0.011120687  y:  0.00012366968\n",
            "step:  257  x:  0.010898273  y:  0.00011877236\n",
            "step:  258  x:  0.010680308  y:  0.00011406897\n",
            "step:  259  x:  0.010466701  y:  0.00010955184\n",
            "step:  260  x:  0.010257367  y:  0.00010521358\n",
            "step:  261  x:  0.01005222  y:  0.000101047124\n",
            "step:  262  x:  0.009851175  y:  9.704565e-05\n",
            "step:  263  x:  0.009654152  y:  9.320265e-05\n",
            "step:  264  x:  0.0094610695  y:  8.9511836e-05\n",
            "step:  265  x:  0.009271848  y:  8.596716e-05\n",
            "step:  266  x:  0.009086411  y:  8.2562874e-05\n",
            "step:  267  x:  0.008904683  y:  7.9293386e-05\n",
            "step:  268  x:  0.008726589  y:  7.615336e-05\n",
            "step:  269  x:  0.008552058  y:  7.313769e-05\n",
            "step:  270  x:  0.008381017  y:  7.024144e-05\n",
            "step:  271  x:  0.008213396  y:  6.745988e-05\n",
            "step:  272  x:  0.008049129  y:  6.478847e-05\n",
            "step:  273  x:  0.007888146  y:  6.222284e-05\n",
            "step:  274  x:  0.007730383  y:  5.9758822e-05\n",
            "step:  275  x:  0.0075757755  y:  5.7392375e-05\n",
            "step:  276  x:  0.00742426  y:  5.5119635e-05\n",
            "step:  277  x:  0.0072757746  y:  5.2936895e-05\n",
            "step:  278  x:  0.007130259  y:  5.0840597e-05\n",
            "step:  279  x:  0.006987654  y:  4.882731e-05\n",
            "step:  280  x:  0.0068479013  y:  4.689375e-05\n",
            "step:  281  x:  0.0067109433  y:  4.503676e-05\n",
            "step:  282  x:  0.0065767244  y:  4.32533e-05\n",
            "step:  283  x:  0.00644519  y:  4.1540472e-05\n",
            "step:  284  x:  0.006316286  y:  3.989547e-05\n",
            "step:  285  x:  0.0061899605  y:  3.831561e-05\n",
            "step:  286  x:  0.006066161  y:  3.6798312e-05\n",
            "step:  287  x:  0.005944838  y:  3.5341098e-05\n",
            "step:  288  x:  0.005825941  y:  3.394159e-05\n",
            "step:  289  x:  0.0057094223  y:  3.25975e-05\n",
            "step:  290  x:  0.0055952338  y:  3.130664e-05\n",
            "step:  291  x:  0.0054833293  y:  3.00669e-05\n",
            "step:  292  x:  0.005373663  y:  2.8876251e-05\n",
            "step:  293  x:  0.0052661896  y:  2.7732753e-05\n",
            "step:  294  x:  0.005160866  y:  2.6634536e-05\n",
            "step:  295  x:  0.0050576488  y:  2.5579811e-05\n",
            "step:  296  x:  0.004956496  y:  2.4566853e-05\n",
            "step:  297  x:  0.004857366  y:  2.3594004e-05\n",
            "step:  298  x:  0.004760219  y:  2.2659682e-05\n",
            "step:  299  x:  0.0046650143  y:  2.1762358e-05\n",
            "step:  300  x:  0.004571714  y:  2.0900568e-05\n",
            "step:  301  x:  0.0044802795  y:  2.0072905e-05\n",
            "step:  302  x:  0.0043906737  y:  1.9278015e-05\n",
            "step:  303  x:  0.0043028602  y:  1.8514605e-05\n",
            "step:  304  x:  0.0042168032  y:  1.7781429e-05\n",
            "step:  305  x:  0.0041324673  y:  1.7077286e-05\n",
            "step:  306  x:  0.004049818  y:  1.6401025e-05\n",
            "step:  307  x:  0.003968822  y:  1.5751546e-05\n",
            "step:  308  x:  0.0038894454  y:  1.5127785e-05\n",
            "step:  309  x:  0.0038116565  y:  1.4528725e-05\n",
            "step:  310  x:  0.0037354233  y:  1.3953388e-05\n",
            "step:  311  x:  0.003660715  y:  1.3400834e-05\n",
            "step:  312  x:  0.0035875007  y:  1.2870161e-05\n",
            "step:  313  x:  0.0035157506  y:  1.2360502e-05\n",
            "step:  314  x:  0.0034454355  y:  1.1871026e-05\n",
            "step:  315  x:  0.0033765268  y:  1.1400933e-05\n",
            "step:  316  x:  0.0033089963  y:  1.0949457e-05\n",
            "step:  317  x:  0.0032428163  y:  1.0515858e-05\n",
            "step:  318  x:  0.00317796  y:  1.0099429e-05\n",
            "step:  319  x:  0.0031144007  y:  9.699492e-06\n",
            "step:  320  x:  0.0030521126  y:  9.315391e-06\n",
            "step:  321  x:  0.0029910705  y:  8.946503e-06\n",
            "step:  322  x:  0.002931249  y:  8.592221e-06\n",
            "step:  323  x:  0.0028726242  y:  8.25197e-06\n",
            "step:  324  x:  0.0028151716  y:  7.925191e-06\n",
            "step:  325  x:  0.0027588683  y:  7.6113542e-06\n",
            "step:  326  x:  0.002703691  y:  7.3099445e-06\n",
            "step:  327  x:  0.0026496171  y:  7.020471e-06\n",
            "step:  328  x:  0.002596625  y:  6.7424608e-06\n",
            "step:  329  x:  0.0025446925  y:  6.47546e-06\n",
            "step:  330  x:  0.0024937987  y:  6.219032e-06\n",
            "step:  331  x:  0.0024439227  y:  5.972758e-06\n",
            "step:  332  x:  0.0023950443  y:  5.736237e-06\n",
            "step:  333  x:  0.0023471434  y:  5.509082e-06\n",
            "step:  334  x:  0.0023002005  y:  5.290922e-06\n",
            "step:  335  x:  0.0022541964  y:  5.0814015e-06\n",
            "step:  336  x:  0.0022091125  y:  4.880178e-06\n",
            "step:  337  x:  0.0021649303  y:  4.686923e-06\n",
            "step:  338  x:  0.0021216318  y:  4.501321e-06\n",
            "step:  339  x:  0.002079199  y:  4.323069e-06\n",
            "step:  340  x:  0.002037615  y:  4.151875e-06\n",
            "step:  341  x:  0.0019968627  y:  3.987461e-06\n",
            "step:  342  x:  0.0019569255  y:  3.8295575e-06\n",
            "step:  343  x:  0.001917787  y:  3.677907e-06\n",
            "step:  344  x:  0.0018794313  y:  3.532262e-06\n",
            "step:  345  x:  0.0018418427  y:  3.3923843e-06\n",
            "step:  346  x:  0.0018050058  y:  3.258046e-06\n",
            "step:  347  x:  0.0017689057  y:  3.1290272e-06\n",
            "step:  348  x:  0.0017335275  y:  3.0051176e-06\n",
            "step:  349  x:  0.0016988569  y:  2.886115e-06\n",
            "step:  350  x:  0.0016648798  y:  2.771825e-06\n",
            "step:  351  x:  0.0016315823  y:  2.6620608e-06\n",
            "step:  352  x:  0.0015989506  y:  2.556643e-06\n",
            "step:  353  x:  0.0015669715  y:  2.4553997e-06\n",
            "step:  354  x:  0.0015356321  y:  2.3581658e-06\n",
            "step:  355  x:  0.0015049194  y:  2.2647823e-06\n",
            "step:  356  x:  0.001474821  y:  2.175097e-06\n",
            "step:  357  x:  0.0014453246  y:  2.0889634e-06\n",
            "step:  358  x:  0.0014164181  y:  2.0062403e-06\n",
            "step:  359  x:  0.0013880897  y:  1.9267932e-06\n",
            "step:  360  x:  0.0013603279  y:  1.8504921e-06\n",
            "step:  361  x:  0.0013331213  y:  1.7772124e-06\n",
            "step:  362  x:  0.001306459  y:  1.7068351e-06\n",
            "step:  363  x:  0.0012803298  y:  1.6392444e-06\n",
            "step:  364  x:  0.0012547232  y:  1.5743302e-06\n",
            "step:  365  x:  0.0012296287  y:  1.5119867e-06\n",
            "step:  366  x:  0.0012050361  y:  1.452112e-06\n",
            "step:  367  x:  0.0011809353  y:  1.3946081e-06\n",
            "step:  368  x:  0.0011573166  y:  1.3393817e-06\n",
            "step:  369  x:  0.0011341703  y:  1.2863424e-06\n",
            "step:  370  x:  0.0011114869  y:  1.2354031e-06\n",
            "step:  371  x:  0.0010892572  y:  1.1864812e-06\n",
            "step:  372  x:  0.001067472  y:  1.1394966e-06\n",
            "step:  373  x:  0.0010461226  y:  1.0943726e-06\n",
            "step:  374  x:  0.0010252002  y:  1.0510355e-06\n",
            "step:  375  x:  0.0010046962  y:  1.0094145e-06\n",
            "step:  376  x:  0.0009846024  y:  9.694419e-07\n",
            "step:  377  x:  0.0009649103  y:  9.3105194e-07\n",
            "step:  378  x:  0.0009456121  y:  8.941822e-07\n",
            "step:  379  x:  0.00092669984  y:  8.587726e-07\n",
            "step:  380  x:  0.0009081658  y:  8.2476515e-07\n",
            "step:  381  x:  0.0008900025  y:  7.921045e-07\n",
            "step:  382  x:  0.00087220245  y:  7.607371e-07\n",
            "step:  383  x:  0.00085475843  y:  7.30612e-07\n",
            "step:  384  x:  0.00083766325  y:  7.016797e-07\n",
            "step:  385  x:  0.00082091  y:  6.7389317e-07\n",
            "step:  386  x:  0.00080449175  y:  6.4720695e-07\n",
            "step:  387  x:  0.0007884019  y:  6.215776e-07\n",
            "step:  388  x:  0.0007726339  y:  5.969631e-07\n",
            "step:  389  x:  0.0007571812  y:  5.7332335e-07\n",
            "step:  390  x:  0.0007420376  y:  5.506198e-07\n",
            "step:  391  x:  0.00072719686  y:  5.288153e-07\n",
            "step:  392  x:  0.0007126529  y:  5.078742e-07\n",
            "step:  393  x:  0.00069839985  y:  4.8776235e-07\n",
            "step:  394  x:  0.00068443187  y:  4.6844698e-07\n",
            "step:  395  x:  0.00067074323  y:  4.4989648e-07\n",
            "step:  396  x:  0.00065732835  y:  4.3208055e-07\n",
            "step:  397  x:  0.0006441818  y:  4.1497017e-07\n",
            "step:  398  x:  0.0006312982  y:  3.985374e-07\n",
            "step:  399  x:  0.00061867223  y:  3.8275533e-07\n",
            "step:  400  x:  0.0006062988  y:  3.6759823e-07\n",
            "step:  401  x:  0.0005941728  y:  3.5304132e-07\n",
            "step:  402  x:  0.00058228936  y:  3.390609e-07\n",
            "step:  403  x:  0.0005706436  y:  3.256341e-07\n",
            "step:  404  x:  0.0005592307  y:  3.1273896e-07\n",
            "step:  405  x:  0.0005480461  y:  3.0035451e-07\n",
            "step:  406  x:  0.0005370852  y:  2.8846048e-07\n",
            "step:  407  x:  0.0005263435  y:  2.7703746e-07\n",
            "step:  408  x:  0.0005158166  y:  2.660668e-07\n",
            "step:  409  x:  0.0005055003  y:  2.5553055e-07\n",
            "step:  410  x:  0.00049539027  y:  2.454115e-07\n",
            "step:  411  x:  0.00048548245  y:  2.3569321e-07\n",
            "step:  412  x:  0.0004757728  y:  2.2635976e-07\n",
            "step:  413  x:  0.00046625733  y:  2.1739591e-07\n",
            "step:  414  x:  0.00045693218  y:  2.0878701e-07\n",
            "step:  415  x:  0.00044779354  y:  2.0051905e-07\n",
            "step:  416  x:  0.00043883768  y:  1.9257851e-07\n",
            "step:  417  x:  0.00043006093  y:  1.849524e-07\n",
            "step:  418  x:  0.0004214597  y:  1.7762828e-07\n",
            "step:  419  x:  0.0004130305  y:  1.7059419e-07\n",
            "step:  420  x:  0.0004047699  y:  1.6383868e-07\n",
            "step:  421  x:  0.0003966745  y:  1.5735066e-07\n",
            "step:  422  x:  0.000388741  y:  1.5111956e-07\n",
            "step:  423  x:  0.00038096617  y:  1.4513522e-07\n",
            "step:  424  x:  0.00037334685  y:  1.3938786e-07\n",
            "step:  425  x:  0.0003658799  y:  1.3386811e-07\n",
            "step:  426  x:  0.0003585623  y:  1.2856692e-07\n",
            "step:  427  x:  0.00035139106  y:  1.2347567e-07\n",
            "step:  428  x:  0.00034436324  y:  1.18586044e-07\n",
            "step:  429  x:  0.00033747597  y:  1.1389003e-07\n",
            "step:  430  x:  0.00033072644  y:  1.0937998e-07\n",
            "step:  431  x:  0.00032411193  y:  1.0504854e-07\n",
            "step:  432  x:  0.0003176297  y:  1.0088862e-07\n",
            "step:  433  x:  0.0003112771  y:  9.689344e-08\n",
            "step:  434  x:  0.00030505157  y:  9.305646e-08\n",
            "step:  435  x:  0.00029895053  y:  8.937142e-08\n",
            "step:  436  x:  0.00029297153  y:  8.583231e-08\n",
            "step:  437  x:  0.0002871121  y:  8.243337e-08\n",
            "step:  438  x:  0.00028136987  y:  7.9169e-08\n",
            "step:  439  x:  0.00027574247  y:  7.6033906e-08\n",
            "step:  440  x:  0.0002702276  y:  7.302296e-08\n",
            "step:  441  x:  0.00026482306  y:  7.013125e-08\n",
            "step:  442  x:  0.0002595266  y:  6.735406e-08\n",
            "step:  443  x:  0.00025433608  y:  6.468684e-08\n",
            "step:  444  x:  0.00024924937  y:  6.2125245e-08\n",
            "step:  445  x:  0.00024426437  y:  5.966508e-08\n",
            "step:  446  x:  0.0002393791  y:  5.730235e-08\n",
            "step:  447  x:  0.00023459151  y:  5.5033176e-08\n",
            "step:  448  x:  0.00022989968  y:  5.2853864e-08\n",
            "step:  449  x:  0.00022530169  y:  5.076085e-08\n",
            "step:  450  x:  0.00022079566  y:  4.875072e-08\n",
            "step:  451  x:  0.00021637975  y:  4.6820194e-08\n",
            "step:  452  x:  0.00021205215  y:  4.4966114e-08\n",
            "step:  453  x:  0.00020781111  y:  4.318546e-08\n",
            "step:  454  x:  0.0002036549  y:  4.1475317e-08\n",
            "step:  455  x:  0.0001995818  y:  3.9832894e-08\n",
            "step:  456  x:  0.00019559017  y:  3.8255514e-08\n",
            "step:  457  x:  0.00019167837  y:  3.6740595e-08\n",
            "step:  458  x:  0.0001878448  y:  3.5285666e-08\n",
            "step:  459  x:  0.0001840879  y:  3.3888355e-08\n",
            "step:  460  x:  0.00018040615  y:  3.2546378e-08\n",
            "step:  461  x:  0.00017679803  y:  3.1257542e-08\n",
            "step:  462  x:  0.00017326207  y:  3.0019745e-08\n",
            "step:  463  x:  0.00016979683  y:  2.8830963e-08\n",
            "step:  464  x:  0.00016640089  y:  2.7689255e-08\n",
            "step:  465  x:  0.00016307287  y:  2.659276e-08\n",
            "step:  466  x:  0.0001598114  y:  2.5539684e-08\n",
            "step:  467  x:  0.00015661518  y:  2.4528314e-08\n",
            "step:  468  x:  0.00015348288  y:  2.3556993e-08\n",
            "step:  469  x:  0.00015041322  y:  2.2624137e-08\n",
            "step:  470  x:  0.00014740496  y:  2.1728223e-08\n",
            "step:  471  x:  0.00014445686  y:  2.0867786e-08\n",
            "step:  472  x:  0.00014156773  y:  2.0041421e-08\n",
            "step:  473  x:  0.00013873637  y:  1.9247782e-08\n",
            "step:  474  x:  0.00013596164  y:  1.8485569e-08\n",
            "step:  475  x:  0.00013324241  y:  1.775354e-08\n",
            "step:  476  x:  0.00013057757  y:  1.7050501e-08\n",
            "step:  477  x:  0.00012796602  y:  1.6375303e-08\n",
            "step:  478  x:  0.0001254067  y:  1.572684e-08\n",
            "step:  479  x:  0.00012289856  y:  1.5104057e-08\n",
            "step:  480  x:  0.00012044059  y:  1.4505936e-08\n",
            "step:  481  x:  0.00011803178  y:  1.39315e-08\n",
            "step:  482  x:  0.000115671144  y:  1.3379814e-08\n",
            "step:  483  x:  0.000113357724  y:  1.28499735e-08\n",
            "step:  484  x:  0.00011109057  y:  1.2341116e-08\n",
            "step:  485  x:  0.000108868764  y:  1.1852408e-08\n",
            "step:  486  x:  0.00010669139  y:  1.1383053e-08\n",
            "step:  487  x:  0.00010455756  y:  1.09322835e-08\n",
            "step:  488  x:  0.00010246641  y:  1.0499365e-08\n",
            "step:  489  x:  0.00010041708  y:  1.0083589e-08\n",
            "step:  490  x:  9.840874e-05  y:  9.68428e-09\n",
            "step:  491  x:  9.644056e-05  y:  9.300782e-09\n",
            "step:  492  x:  9.451175e-05  y:  8.932471e-09\n",
            "step:  493  x:  9.2621514e-05  y:  8.578745e-09\n",
            "step:  494  x:  9.0769085e-05  y:  8.239026e-09\n",
            "step:  495  x:  8.8953704e-05  y:  7.912761e-09\n",
            "step:  496  x:  8.717463e-05  y:  7.599416e-09\n",
            "step:  497  x:  8.543114e-05  y:  7.298479e-09\n",
            "step:  498  x:  8.3722516e-05  y:  7.0094597e-09\n",
            "step:  499  x:  8.204806e-05  y:  6.731885e-09\n",
            "step:  500  x:  8.04071e-05  y:  6.465302e-09\n",
            "step:  501  x:  7.879896e-05  y:  6.2092766e-09\n",
            "step:  502  x:  7.722298e-05  y:  5.9633893e-09\n",
            "step:  503  x:  7.567852e-05  y:  5.7272387e-09\n",
            "step:  504  x:  7.416495e-05  y:  5.5004397e-09\n",
            "step:  505  x:  7.268165e-05  y:  5.2826223e-09\n",
            "step:  506  x:  7.1228016e-05  y:  5.07343e-09\n",
            "step:  507  x:  6.980346e-05  y:  4.8725224e-09\n",
            "step:  508  x:  6.840739e-05  y:  4.679571e-09\n",
            "step:  509  x:  6.703924e-05  y:  4.4942596e-09\n",
            "step:  510  x:  6.5698456e-05  y:  4.316287e-09\n",
            "step:  511  x:  6.438448e-05  y:  4.145362e-09\n",
            "step:  512  x:  6.309679e-05  y:  3.981205e-09\n",
            "step:  513  x:  6.183486e-05  y:  3.8235495e-09\n",
            "step:  514  x:  6.059816e-05  y:  3.6721368e-09\n",
            "step:  515  x:  5.9386195e-05  y:  3.5267202e-09\n",
            "step:  516  x:  5.819847e-05  y:  3.387062e-09\n",
            "step:  517  x:  5.7034504e-05  y:  3.2529346e-09\n",
            "step:  518  x:  5.5893815e-05  y:  3.1241185e-09\n",
            "step:  519  x:  5.477594e-05  y:  3.0004037e-09\n",
            "step:  520  x:  5.3680422e-05  y:  2.8815876e-09\n",
            "step:  521  x:  5.2606814e-05  y:  2.767477e-09\n",
            "step:  522  x:  5.1554678e-05  y:  2.6578848e-09\n",
            "step:  523  x:  5.0523584e-05  y:  2.5526325e-09\n",
            "step:  524  x:  4.9513113e-05  y:  2.4515483e-09\n",
            "step:  525  x:  4.8522852e-05  y:  2.354467e-09\n",
            "step:  526  x:  4.7552396e-05  y:  2.2612303e-09\n",
            "step:  527  x:  4.6601348e-05  y:  2.1716857e-09\n",
            "step:  528  x:  4.566932e-05  y:  2.0856867e-09\n",
            "step:  529  x:  4.4755932e-05  y:  2.0030935e-09\n",
            "step:  530  x:  4.3860815e-05  y:  1.9237711e-09\n",
            "step:  531  x:  4.29836e-05  y:  1.8475899e-09\n",
            "step:  532  x:  4.2123927e-05  y:  1.7744253e-09\n",
            "step:  533  x:  4.1281448e-05  y:  1.7041579e-09\n",
            "step:  534  x:  4.045582e-05  y:  1.6366734e-09\n",
            "step:  535  x:  3.9646704e-05  y:  1.5718611e-09\n",
            "step:  536  x:  3.885377e-05  y:  1.5096154e-09\n",
            "step:  537  x:  3.8076694e-05  y:  1.4498346e-09\n",
            "step:  538  x:  3.731516e-05  y:  1.3924212e-09\n",
            "step:  539  x:  3.6568857e-05  y:  1.3372813e-09\n",
            "step:  540  x:  3.5837482e-05  y:  1.2843251e-09\n",
            "step:  541  x:  3.512073e-05  y:  1.2334658e-09\n",
            "step:  542  x:  3.4418317e-05  y:  1.1846205e-09\n",
            "step:  543  x:  3.372995e-05  y:  1.1377095e-09\n",
            "step:  544  x:  3.3055352e-05  y:  1.0926563e-09\n",
            "step:  545  x:  3.2394244e-05  y:  1.049387e-09\n",
            "step:  546  x:  3.174636e-05  y:  1.0078314e-09\n",
            "step:  547  x:  3.1111435e-05  y:  9.679214e-10\n",
            "step:  548  x:  3.0489206e-05  y:  9.295917e-10\n",
            "step:  549  x:  2.9879422e-05  y:  8.9277985e-10\n",
            "step:  550  x:  2.9281833e-05  y:  8.5742574e-10\n",
            "step:  551  x:  2.8696197e-05  y:  8.2347174e-10\n",
            "step:  552  x:  2.8122273e-05  y:  7.908622e-10\n",
            "step:  553  x:  2.7559827e-05  y:  7.5954404e-10\n",
            "step:  554  x:  2.7008631e-05  y:  7.2946615e-10\n",
            "step:  555  x:  2.6468459e-05  y:  7.005793e-10\n",
            "step:  556  x:  2.5939089e-05  y:  6.7283634e-10\n",
            "step:  557  x:  2.5420308e-05  y:  6.4619204e-10\n",
            "step:  558  x:  2.4911902e-05  y:  6.2060285e-10\n",
            "step:  559  x:  2.4413665e-05  y:  5.96027e-10\n",
            "step:  560  x:  2.3925391e-05  y:  5.7242433e-10\n",
            "step:  561  x:  2.3446884e-05  y:  5.497564e-10\n",
            "step:  562  x:  2.2977947e-05  y:  5.2798604e-10\n",
            "step:  563  x:  2.2518389e-05  y:  5.070778e-10\n",
            "step:  564  x:  2.2068021e-05  y:  4.8699755e-10\n",
            "step:  565  x:  2.1626662e-05  y:  4.677125e-10\n",
            "step:  566  x:  2.1194128e-05  y:  4.4919105e-10\n",
            "step:  567  x:  2.0770245e-05  y:  4.3140308e-10\n",
            "step:  568  x:  2.035484e-05  y:  4.1431955e-10\n",
            "step:  569  x:  1.9947744e-05  y:  3.9791248e-10\n",
            "step:  570  x:  1.9548788e-05  y:  3.821551e-10\n",
            "step:  571  x:  1.9157813e-05  y:  3.670218e-10\n",
            "step:  572  x:  1.8774657e-05  y:  3.5248776e-10\n",
            "step:  573  x:  1.8399163e-05  y:  3.385292e-10\n",
            "step:  574  x:  1.803118e-05  y:  3.2512346e-10\n",
            "step:  575  x:  1.7670556e-05  y:  3.1224856e-10\n",
            "step:  576  x:  1.7317145e-05  y:  2.998835e-10\n",
            "step:  577  x:  1.6970802e-05  y:  2.8800812e-10\n",
            "step:  578  x:  1.6631386e-05  y:  2.76603e-10\n",
            "step:  579  x:  1.6298758e-05  y:  2.656495e-10\n",
            "step:  580  x:  1.5972782e-05  y:  2.5512978e-10\n",
            "step:  581  x:  1.5653326e-05  y:  2.450266e-10\n",
            "step:  582  x:  1.534026e-05  y:  2.3532357e-10\n",
            "step:  583  x:  1.50334545e-05  y:  2.2600476e-10\n",
            "step:  584  x:  1.47327855e-05  y:  2.1705497e-10\n",
            "step:  585  x:  1.443813e-05  y:  2.084596e-10\n",
            "step:  586  x:  1.4149367e-05  y:  2.002046e-10\n",
            "step:  587  x:  1.386638e-05  y:  1.9227649e-10\n",
            "step:  588  x:  1.3589052e-05  y:  1.8466234e-10\n",
            "step:  589  x:  1.3317272e-05  y:  1.7734972e-10\n",
            "step:  590  x:  1.3050926e-05  y:  1.7032667e-10\n",
            "step:  591  x:  1.27899075e-05  y:  1.6358173e-10\n",
            "step:  592  x:  1.2534109e-05  y:  1.571039e-10\n",
            "step:  593  x:  1.2283427e-05  y:  1.5088258e-10\n",
            "step:  594  x:  1.2037759e-05  y:  1.4490764e-10\n",
            "step:  595  x:  1.1797004e-05  y:  1.391693e-10\n",
            "step:  596  x:  1.1561064e-05  y:  1.336582e-10\n",
            "step:  597  x:  1.1329843e-05  y:  1.2836535e-10\n",
            "step:  598  x:  1.1103246e-05  y:  1.2328207e-10\n",
            "step:  599  x:  1.0881181e-05  y:  1.184001e-10\n",
            "step:  600  x:  1.0663557e-05  y:  1.1371145e-10\n",
            "step:  601  x:  1.0450286e-05  y:  1.09208476e-10\n",
            "step:  602  x:  1.02412805e-05  y:  1.04883824e-10\n",
            "step:  603  x:  1.0036455e-05  y:  1.0073043e-10\n",
            "step:  604  x:  9.835726e-06  y:  9.6741504e-11\n",
            "step:  605  x:  9.639011e-06  y:  9.291054e-11\n",
            "step:  606  x:  9.446231e-06  y:  8.923128e-11\n",
            "step:  607  x:  9.257306e-06  y:  8.569772e-11\n",
            "step:  608  x:  9.0721605e-06  y:  8.2304094e-11\n",
            "step:  609  x:  8.890717e-06  y:  7.9044854e-11\n",
            "step:  610  x:  8.712903e-06  y:  7.591468e-11\n",
            "step:  611  x:  8.538645e-06  y:  7.290845e-11\n",
            "step:  612  x:  8.3678715e-06  y:  7.0021274e-11\n",
            "step:  613  x:  8.2005145e-06  y:  6.7248436e-11\n",
            "step:  614  x:  8.036504e-06  y:  6.45854e-11\n",
            "step:  615  x:  7.875774e-06  y:  6.202781e-11\n",
            "step:  616  x:  7.7182585e-06  y:  5.9571514e-11\n",
            "step:  617  x:  7.563893e-06  y:  5.721248e-11\n",
            "step:  618  x:  7.412615e-06  y:  5.4946863e-11\n",
            "step:  619  x:  7.264363e-06  y:  5.2770968e-11\n",
            "step:  620  x:  7.1190757e-06  y:  5.0681237e-11\n",
            "step:  621  x:  6.9766943e-06  y:  4.8674263e-11\n",
            "step:  622  x:  6.8371605e-06  y:  4.6746763e-11\n",
            "step:  623  x:  6.7004175e-06  y:  4.4895594e-11\n",
            "step:  624  x:  6.5664094e-06  y:  4.311773e-11\n",
            "step:  625  x:  6.435081e-06  y:  4.1410268e-11\n",
            "step:  626  x:  6.3063794e-06  y:  3.9770423e-11\n",
            "step:  627  x:  6.180252e-06  y:  3.8195516e-11\n",
            "step:  628  x:  6.056647e-06  y:  3.6682976e-11\n",
            "step:  629  x:  5.9355143e-06  y:  3.523033e-11\n",
            "step:  630  x:  5.816804e-06  y:  3.383521e-11\n",
            "step:  631  x:  5.7004677e-06  y:  3.2495333e-11\n",
            "step:  632  x:  5.5864584e-06  y:  3.120852e-11\n",
            "step:  633  x:  5.4747293e-06  y:  2.997266e-11\n",
            "step:  634  x:  5.3652348e-06  y:  2.8785745e-11\n",
            "step:  635  x:  5.25793e-06  y:  2.7645827e-11\n",
            "step:  636  x:  5.1527713e-06  y:  2.6551053e-11\n",
            "step:  637  x:  5.049716e-06  y:  2.5499632e-11\n",
            "step:  638  x:  4.9487217e-06  y:  2.4489846e-11\n",
            "step:  639  x:  4.8497473e-06  y:  2.3520049e-11\n",
            "step:  640  x:  4.7527524e-06  y:  2.2588655e-11\n",
            "step:  641  x:  4.6576974e-06  y:  2.1694145e-11\n",
            "step:  642  x:  4.5645434e-06  y:  2.0835056e-11\n",
            "step:  643  x:  4.4732524e-06  y:  2.0009987e-11\n",
            "step:  644  x:  4.383787e-06  y:  1.9217591e-11\n",
            "step:  645  x:  4.2961115e-06  y:  1.8456573e-11\n",
            "step:  646  x:  4.210189e-06  y:  1.7725692e-11\n",
            "step:  647  x:  4.1259855e-06  y:  1.7023756e-11\n",
            "step:  648  x:  4.0434657e-06  y:  1.6349614e-11\n",
            "step:  649  x:  3.9625966e-06  y:  1.5702172e-11\n",
            "step:  650  x:  3.8833446e-06  y:  1.5080366e-11\n",
            "step:  651  x:  3.8056776e-06  y:  1.4483182e-11\n",
            "step:  652  x:  3.729564e-06  y:  1.3909648e-11\n",
            "step:  653  x:  3.6549727e-06  y:  1.3358825e-11\n",
            "step:  654  x:  3.5818732e-06  y:  1.2829815e-11\n",
            "step:  655  x:  3.5102357e-06  y:  1.2321755e-11\n",
            "step:  656  x:  3.440031e-06  y:  1.1833813e-11\n",
            "step:  657  x:  3.3712304e-06  y:  1.1365194e-11\n",
            "step:  658  x:  3.3038057e-06  y:  1.0915132e-11\n",
            "step:  659  x:  3.2377295e-06  y:  1.0482892e-11\n",
            "step:  660  x:  3.1729749e-06  y:  1.00677695e-11\n",
            "step:  661  x:  3.1095153e-06  y:  9.669086e-12\n",
            "step:  662  x:  3.047325e-06  y:  9.28619e-12\n",
            "step:  663  x:  2.9863784e-06  y:  8.918455e-12\n",
            "step:  664  x:  2.9266507e-06  y:  8.565285e-12\n",
            "step:  665  x:  2.8681177e-06  y:  8.2260995e-12\n",
            "step:  666  x:  2.8107554e-06  y:  7.900346e-12\n",
            "step:  667  x:  2.7545402e-06  y:  7.587491e-12\n",
            "step:  668  x:  2.6994494e-06  y:  7.287027e-12\n",
            "step:  669  x:  2.6454604e-06  y:  6.998461e-12\n",
            "step:  670  x:  2.5925513e-06  y:  6.721322e-12\n",
            "step:  671  x:  2.5407003e-06  y:  6.455158e-12\n",
            "step:  672  x:  2.4898864e-06  y:  6.199534e-12\n",
            "step:  673  x:  2.4400886e-06  y:  5.9540324e-12\n",
            "step:  674  x:  2.391287e-06  y:  5.718253e-12\n",
            "step:  675  x:  2.3434611e-06  y:  5.49181e-12\n",
            "step:  676  x:  2.296592e-06  y:  5.2743344e-12\n",
            "step:  677  x:  2.2506601e-06  y:  5.065471e-12\n",
            "step:  678  x:  2.205647e-06  y:  4.8648785e-12\n",
            "step:  679  x:  2.161534e-06  y:  4.672229e-12\n",
            "step:  680  x:  2.1183032e-06  y:  4.4872083e-12\n",
            "step:  681  x:  2.0759371e-06  y:  4.309515e-12\n",
            "step:  682  x:  2.0344185e-06  y:  4.1388585e-12\n",
            "step:  683  x:  1.9937302e-06  y:  3.97496e-12\n",
            "step:  684  x:  1.9538556e-06  y:  3.8175517e-12\n",
            "step:  685  x:  1.9147785e-06  y:  3.6663767e-12\n",
            "step:  686  x:  1.876483e-06  y:  3.5211883e-12\n",
            "step:  687  x:  1.8389534e-06  y:  3.3817495e-12\n",
            "step:  688  x:  1.8021743e-06  y:  3.2478323e-12\n",
            "step:  689  x:  1.7661308e-06  y:  3.119218e-12\n",
            "step:  690  x:  1.7308082e-06  y:  2.995697e-12\n",
            "step:  691  x:  1.696192e-06  y:  2.8770675e-12\n",
            "step:  692  x:  1.6622682e-06  y:  2.7631356e-12\n",
            "step:  693  x:  1.6290229e-06  y:  2.6537156e-12\n",
            "step:  694  x:  1.5964424e-06  y:  2.5486284e-12\n",
            "step:  695  x:  1.5645136e-06  y:  2.4477028e-12\n",
            "step:  696  x:  1.5332233e-06  y:  2.3507739e-12\n",
            "step:  697  x:  1.5025589e-06  y:  2.2576832e-12\n",
            "step:  698  x:  1.4725077e-06  y:  2.168279e-12\n",
            "step:  699  x:  1.4430576e-06  y:  2.0824152e-12\n",
            "step:  700  x:  1.4141965e-06  y:  1.9999519e-12\n",
            "step:  701  x:  1.3859126e-06  y:  1.9207537e-12\n",
            "step:  702  x:  1.3581944e-06  y:  1.844692e-12\n",
            "step:  703  x:  1.3310305e-06  y:  1.7716422e-12\n",
            "step:  704  x:  1.3044099e-06  y:  1.7014852e-12\n",
            "step:  705  x:  1.2783217e-06  y:  1.6341065e-12\n",
            "step:  706  x:  1.2527553e-06  y:  1.5693958e-12\n",
            "step:  707  x:  1.2277002e-06  y:  1.5072478e-12\n",
            "step:  708  x:  1.2031462e-06  y:  1.4475608e-12\n",
            "step:  709  x:  1.1790833e-06  y:  1.3902373e-12\n",
            "step:  710  x:  1.1555015e-06  y:  1.3351838e-12\n",
            "step:  711  x:  1.1323915e-06  y:  1.2823105e-12\n",
            "step:  712  x:  1.1097437e-06  y:  1.2315312e-12\n",
            "step:  713  x:  1.0875489e-06  y:  1.1827625e-12\n",
            "step:  714  x:  1.0657978e-06  y:  1.135925e-12\n",
            "step:  715  x:  1.0444819e-06  y:  1.0909424e-12\n",
            "step:  716  x:  1.0235923e-06  y:  1.0477412e-12\n",
            "step:  717  x:  1.0031205e-06  y:  1.0062506e-12\n",
            "step:  718  x:  9.83058e-07  y:  9.664031e-13\n",
            "step:  719  x:  9.633969e-07  y:  9.281337e-13\n",
            "step:  720  x:  9.44129e-07  y:  8.913796e-13\n",
            "step:  721  x:  9.252464e-07  y:  8.5608094e-13\n",
            "step:  722  x:  9.067415e-07  y:  8.221801e-13\n",
            "step:  723  x:  8.886067e-07  y:  7.8962184e-13\n",
            "step:  724  x:  8.708345e-07  y:  7.583528e-13\n",
            "step:  725  x:  8.534178e-07  y:  7.2832197e-13\n",
            "step:  726  x:  8.3634944e-07  y:  6.994804e-13\n",
            "step:  727  x:  8.1962247e-07  y:  6.71781e-13\n",
            "step:  728  x:  8.0323e-07  y:  6.4517846e-13\n",
            "step:  729  x:  7.8716545e-07  y:  6.1962946e-13\n",
            "step:  730  x:  7.7142215e-07  y:  5.950921e-13\n",
            "step:  731  x:  7.559937e-07  y:  5.715265e-13\n",
            "step:  732  x:  7.408738e-07  y:  5.48894e-13\n",
            "step:  733  x:  7.260563e-07  y:  5.271578e-13\n",
            "step:  734  x:  7.115352e-07  y:  5.0628235e-13\n",
            "step:  735  x:  6.9730453e-07  y:  4.862336e-13\n",
            "step:  736  x:  6.8335845e-07  y:  4.669788e-13\n",
            "step:  737  x:  6.696913e-07  y:  4.4848644e-13\n",
            "step:  738  x:  6.5629746e-07  y:  4.3072636e-13\n",
            "step:  739  x:  6.431715e-07  y:  4.1366961e-13\n",
            "step:  740  x:  6.303081e-07  y:  3.9728827e-13\n",
            "step:  741  x:  6.177019e-07  y:  3.8155565e-13\n",
            "step:  742  x:  6.0534785e-07  y:  3.6644602e-13\n",
            "step:  743  x:  5.932409e-07  y:  3.5193474e-13\n",
            "step:  744  x:  5.8137607e-07  y:  3.3799813e-13\n",
            "step:  745  x:  5.697485e-07  y:  3.2461338e-13\n",
            "step:  746  x:  5.583536e-07  y:  3.1175873e-13\n",
            "step:  747  x:  5.471865e-07  y:  2.9941308e-13\n",
            "step:  748  x:  5.362428e-07  y:  2.8755633e-13\n",
            "step:  749  x:  5.255179e-07  y:  2.761691e-13\n",
            "step:  750  x:  5.1500757e-07  y:  2.652328e-13\n",
            "step:  751  x:  5.0470743e-07  y:  2.547296e-13\n",
            "step:  752  x:  4.946133e-07  y:  2.446423e-13\n",
            "step:  753  x:  4.84721e-07  y:  2.3495444e-13\n",
            "step:  754  x:  4.7502658e-07  y:  2.2565025e-13\n",
            "step:  755  x:  4.6552606e-07  y:  2.167145e-13\n",
            "step:  756  x:  4.5621553e-07  y:  2.0813261e-13\n",
            "step:  757  x:  4.4709122e-07  y:  1.9989056e-13\n",
            "step:  758  x:  4.381494e-07  y:  1.9197491e-13\n",
            "step:  759  x:  4.2938643e-07  y:  1.8437271e-13\n",
            "step:  760  x:  4.207987e-07  y:  1.7707155e-13\n",
            "step:  761  x:  4.1238272e-07  y:  1.7005951e-13\n",
            "step:  762  x:  4.0413505e-07  y:  1.6332514e-13\n",
            "step:  763  x:  3.9605234e-07  y:  1.5685746e-13\n",
            "step:  764  x:  3.881313e-07  y:  1.5064591e-13\n",
            "step:  765  x:  3.8036868e-07  y:  1.4468033e-13\n",
            "step:  766  x:  3.727613e-07  y:  1.3895098e-13\n",
            "step:  767  x:  3.6530608e-07  y:  1.3344854e-13\n",
            "step:  768  x:  3.5799997e-07  y:  1.2816397e-13\n",
            "step:  769  x:  3.5083997e-07  y:  1.2308869e-13\n",
            "step:  770  x:  3.4382316e-07  y:  1.1821437e-13\n",
            "step:  771  x:  3.369467e-07  y:  1.1353308e-13\n",
            "step:  772  x:  3.3020777e-07  y:  1.0903717e-13\n",
            "step:  773  x:  3.2360362e-07  y:  1.047193e-13\n",
            "step:  774  x:  3.1713154e-07  y:  1.0057241e-13\n",
            "step:  775  x:  3.1078892e-07  y:  9.6589756e-14\n",
            "step:  776  x:  3.0457315e-07  y:  9.2764805e-14\n",
            "step:  777  x:  2.984817e-07  y:  8.9091325e-14\n",
            "step:  778  x:  2.9251206e-07  y:  8.5563304e-14\n",
            "step:  779  x:  2.8666182e-07  y:  8.2174996e-14\n",
            "step:  780  x:  2.809286e-07  y:  7.892087e-14\n",
            "step:  781  x:  2.7531001e-07  y:  7.5795605e-14\n",
            "step:  782  x:  2.6980382e-07  y:  7.27941e-14\n",
            "step:  783  x:  2.6440773e-07  y:  6.991145e-14\n",
            "step:  784  x:  2.591196e-07  y:  6.714296e-14\n",
            "step:  785  x:  2.539372e-07  y:  6.44841e-14\n",
            "step:  786  x:  2.4885847e-07  y:  6.1930536e-14\n",
            "step:  787  x:  2.438813e-07  y:  5.9478084e-14\n",
            "step:  788  x:  2.3900367e-07  y:  5.712276e-14\n",
            "step:  789  x:  2.3422359e-07  y:  5.486069e-14\n",
            "step:  790  x:  2.2953913e-07  y:  5.268821e-14\n",
            "step:  791  x:  2.2494834e-07  y:  5.0601755e-14\n",
            "step:  792  x:  2.2044937e-07  y:  4.8597922e-14\n",
            "step:  793  x:  2.1604038e-07  y:  4.6673443e-14\n",
            "step:  794  x:  2.1171957e-07  y:  4.4825173e-14\n",
            "step:  795  x:  2.0748517e-07  y:  4.3050097e-14\n",
            "step:  796  x:  2.0333547e-07  y:  4.1345315e-14\n",
            "step:  797  x:  1.9926877e-07  y:  3.970804e-14\n",
            "step:  798  x:  1.9528339e-07  y:  3.8135602e-14\n",
            "step:  799  x:  1.9137772e-07  y:  3.6625434e-14\n",
            "step:  800  x:  1.8755017e-07  y:  3.5175066e-14\n",
            "step:  801  x:  1.8379917e-07  y:  3.3782137e-14\n",
            "step:  802  x:  1.801232e-07  y:  3.2444364e-14\n",
            "step:  803  x:  1.7652073e-07  y:  3.1159567e-14\n",
            "step:  804  x:  1.7299031e-07  y:  2.9925647e-14\n",
            "step:  805  x:  1.6953051e-07  y:  2.8740594e-14\n",
            "step:  806  x:  1.661399e-07  y:  2.7602466e-14\n",
            "step:  807  x:  1.628171e-07  y:  2.6509409e-14\n",
            "step:  808  x:  1.5956076e-07  y:  2.5459635e-14\n",
            "step:  809  x:  1.5636954e-07  y:  2.4451432e-14\n",
            "step:  810  x:  1.5324214e-07  y:  2.3483153e-14\n",
            "step:  811  x:  1.501773e-07  y:  2.2553221e-14\n",
            "step:  812  x:  1.4717375e-07  y:  2.1660113e-14\n",
            "step:  813  x:  1.4423027e-07  y:  2.080237e-14\n",
            "step:  814  x:  1.4134567e-07  y:  1.9978597e-14\n",
            "step:  815  x:  1.3851876e-07  y:  1.9187446e-14\n",
            "step:  816  x:  1.3574838e-07  y:  1.8427622e-14\n",
            "step:  817  x:  1.3303341e-07  y:  1.7697888e-14\n",
            "step:  818  x:  1.3037274e-07  y:  1.6997051e-14\n",
            "step:  819  x:  1.2776529e-07  y:  1.632397e-14\n",
            "step:  820  x:  1.2520998e-07  y:  1.567754e-14\n",
            "step:  821  x:  1.2270579e-07  y:  1.505671e-14\n",
            "step:  822  x:  1.2025167e-07  y:  1.4460463e-14\n",
            "step:  823  x:  1.1784664e-07  y:  1.388783e-14\n",
            "step:  824  x:  1.15489705e-07  y:  1.3337872e-14\n",
            "step:  825  x:  1.1317991e-07  y:  1.2809692e-14\n",
            "step:  826  x:  1.1091631e-07  y:  1.2302428e-14\n",
            "step:  827  x:  1.0869798e-07  y:  1.18152516e-14\n",
            "step:  828  x:  1.0652402e-07  y:  1.1347367e-14\n",
            "step:  829  x:  1.0439354e-07  y:  1.08980114e-14\n",
            "step:  830  x:  1.0230567e-07  y:  1.046645e-14\n",
            "step:  831  x:  1.00259555e-07  y:  1.0051979e-14\n",
            "step:  832  x:  9.825436e-08  y:  9.653919e-15\n",
            "step:  833  x:  9.628928e-08  y:  9.271625e-15\n",
            "step:  834  x:  9.436349e-08  y:  8.904469e-15\n",
            "step:  835  x:  9.247622e-08  y:  8.551851e-15\n",
            "step:  836  x:  9.06267e-08  y:  8.213198e-15\n",
            "step:  837  x:  8.881417e-08  y:  7.887956e-15\n",
            "step:  838  x:  8.703788e-08  y:  7.575593e-15\n",
            "step:  839  x:  8.5297124e-08  y:  7.2756e-15\n",
            "step:  840  x:  8.359118e-08  y:  6.9874856e-15\n",
            "step:  841  x:  8.191936e-08  y:  6.7107816e-15\n",
            "step:  842  x:  8.028098e-08  y:  6.445035e-15\n",
            "step:  843  x:  7.8675356e-08  y:  6.189812e-15\n",
            "step:  844  x:  7.710185e-08  y:  5.9446958e-15\n",
            "step:  845  x:  7.555982e-08  y:  5.7092862e-15\n",
            "step:  846  x:  7.404862e-08  y:  5.4831983e-15\n",
            "step:  847  x:  7.256765e-08  y:  5.266064e-15\n",
            "step:  848  x:  7.11163e-08  y:  5.057528e-15\n",
            "step:  849  x:  6.969397e-08  y:  4.857249e-15\n",
            "step:  850  x:  6.830009e-08  y:  4.6649022e-15\n",
            "step:  851  x:  6.6934085e-08  y:  4.4801716e-15\n",
            "step:  852  x:  6.55954e-08  y:  4.3027567e-15\n",
            "step:  853  x:  6.428349e-08  y:  4.132367e-15\n",
            "step:  854  x:  6.299782e-08  y:  3.9687254e-15\n",
            "step:  855  x:  6.1737865e-08  y:  3.811564e-15\n",
            "step:  856  x:  6.0503105e-08  y:  3.6606257e-15\n",
            "step:  857  x:  5.9293043e-08  y:  3.515665e-15\n",
            "step:  858  x:  5.8107183e-08  y:  3.3764446e-15\n",
            "step:  859  x:  5.694504e-08  y:  3.2427377e-15\n",
            "step:  860  x:  5.580614e-08  y:  3.1143252e-15\n",
            "step:  861  x:  5.4690016e-08  y:  2.9909979e-15\n",
            "step:  862  x:  5.3596217e-08  y:  2.8725545e-15\n",
            "step:  863  x:  5.2524292e-08  y:  2.7588013e-15\n",
            "step:  864  x:  5.147381e-08  y:  2.649553e-15\n",
            "step:  865  x:  5.044433e-08  y:  2.5446305e-15\n",
            "step:  866  x:  4.9435446e-08  y:  2.4438633e-15\n",
            "step:  867  x:  4.8446736e-08  y:  2.3470864e-15\n",
            "step:  868  x:  4.74778e-08  y:  2.2541417e-15\n",
            "step:  869  x:  4.6528246e-08  y:  2.1648777e-15\n",
            "step:  870  x:  4.559768e-08  y:  2.0791485e-15\n",
            "step:  871  x:  4.4685727e-08  y:  1.996814e-15\n",
            "step:  872  x:  4.3792014e-08  y:  1.9177404e-15\n",
            "step:  873  x:  4.2916174e-08  y:  1.841798e-15\n",
            "step:  874  x:  4.205785e-08  y:  1.7688626e-15\n",
            "step:  875  x:  4.121669e-08  y:  1.6988155e-15\n",
            "step:  876  x:  4.0392358e-08  y:  1.6315426e-15\n",
            "step:  877  x:  3.958451e-08  y:  1.5669334e-15\n",
            "step:  878  x:  3.879282e-08  y:  1.5048829e-15\n",
            "step:  879  x:  3.8016964e-08  y:  1.4452896e-15\n",
            "step:  880  x:  3.7256626e-08  y:  1.3880562e-15\n",
            "step:  881  x:  3.6511494e-08  y:  1.3330893e-15\n",
            "step:  882  x:  3.5781266e-08  y:  1.280299e-15\n",
            "step:  883  x:  3.506564e-08  y:  1.2295991e-15\n",
            "step:  884  x:  3.4364326e-08  y:  1.180907e-15\n",
            "step:  885  x:  3.367704e-08  y:  1.134143e-15\n",
            "step:  886  x:  3.30035e-08  y:  1.0892309e-15\n",
            "step:  887  x:  3.234343e-08  y:  1.0460974e-15\n",
            "step:  888  x:  3.1696562e-08  y:  1.004672e-15\n",
            "step:  889  x:  3.106263e-08  y:  9.64887e-16\n",
            "step:  890  x:  3.0441377e-08  y:  9.266774e-16\n",
            "step:  891  x:  2.983255e-08  y:  8.89981e-16\n",
            "step:  892  x:  2.9235897e-08  y:  8.5473766e-16\n",
            "step:  893  x:  2.865118e-08  y:  8.2089007e-16\n",
            "step:  894  x:  2.8078155e-08  y:  7.883828e-16\n",
            "step:  895  x:  2.7516592e-08  y:  7.571628e-16\n",
            "step:  896  x:  2.696626e-08  y:  7.271792e-16\n",
            "step:  897  x:  2.6426935e-08  y:  6.983829e-16\n",
            "step:  898  x:  2.5898396e-08  y:  6.7072696e-16\n",
            "step:  899  x:  2.5380428e-08  y:  6.441661e-16\n",
            "step:  900  x:  2.487282e-08  y:  6.186572e-16\n",
            "step:  901  x:  2.4375364e-08  y:  5.9415835e-16\n",
            "step:  902  x:  2.3887857e-08  y:  5.7062974e-16\n",
            "step:  903  x:  2.34101e-08  y:  5.480328e-16\n",
            "step:  904  x:  2.2941899e-08  y:  5.263307e-16\n",
            "step:  905  x:  2.248306e-08  y:  5.0548803e-16\n",
            "step:  906  x:  2.2033399e-08  y:  4.854707e-16\n",
            "step:  907  x:  2.1592731e-08  y:  4.6624606e-16\n",
            "step:  908  x:  2.1160876e-08  y:  4.4778264e-16\n",
            "step:  909  x:  2.0737659e-08  y:  4.3005049e-16\n",
            "step:  910  x:  2.0322906e-08  y:  4.1302052e-16\n",
            "step:  911  x:  1.9916447e-08  y:  3.9666487e-16\n",
            "step:  912  x:  1.9518119e-08  y:  3.8095696e-16\n",
            "step:  913  x:  1.9127757e-08  y:  3.658711e-16\n",
            "step:  914  x:  1.8745203e-08  y:  3.5138262e-16\n",
            "step:  915  x:  1.8370299e-08  y:  3.3746788e-16\n",
            "step:  916  x:  1.8002893e-08  y:  3.2410416e-16\n",
            "step:  917  x:  1.7642835e-08  y:  3.1126963e-16\n",
            "step:  918  x:  1.7289977e-08  y:  2.989433e-16\n",
            "step:  919  x:  1.6944178e-08  y:  2.8710515e-16\n",
            "step:  920  x:  1.6605295e-08  y:  2.7573583e-16\n",
            "step:  921  x:  1.6273189e-08  y:  2.6481667e-16\n",
            "step:  922  x:  1.5947725e-08  y:  2.5432993e-16\n",
            "step:  923  x:  1.5628771e-08  y:  2.442585e-16\n",
            "step:  924  x:  1.5316196e-08  y:  2.3458588e-16\n",
            "step:  925  x:  1.5009872e-08  y:  2.2529626e-16\n",
            "step:  926  x:  1.4709675e-08  y:  2.1637454e-16\n",
            "step:  927  x:  1.4415482e-08  y:  2.0780613e-16\n",
            "step:  928  x:  1.4127172e-08  y:  1.99577e-16\n",
            "step:  929  x:  1.38446286e-08  y:  1.9167374e-16\n",
            "step:  930  x:  1.3567736e-08  y:  1.8408347e-16\n",
            "step:  931  x:  1.3296382e-08  y:  1.7679376e-16\n",
            "step:  932  x:  1.3030454e-08  y:  1.6979273e-16\n",
            "step:  933  x:  1.2769845e-08  y:  1.6306893e-16\n",
            "step:  934  x:  1.2514448e-08  y:  1.5661142e-16\n",
            "step:  935  x:  1.22641595e-08  y:  1.5040961e-16\n",
            "step:  936  x:  1.2018877e-08  y:  1.444534e-16\n",
            "step:  937  x:  1.1778499e-08  y:  1.3873304e-16\n",
            "step:  938  x:  1.1542929e-08  y:  1.3323922e-16\n",
            "step:  939  x:  1.131207e-08  y:  1.2796293e-16\n",
            "step:  940  x:  1.1085829e-08  y:  1.2289561e-16\n",
            "step:  941  x:  1.0864112e-08  y:  1.1802893e-16\n",
            "step:  942  x:  1.064683e-08  y:  1.1335499e-16\n",
            "step:  943  x:  1.0433894e-08  y:  1.0886614e-16\n",
            "step:  944  x:  1.0225216e-08  y:  1.0455505e-16\n",
            "step:  945  x:  1.0020712e-08  y:  1.0041468e-16\n",
            "step:  946  x:  9.820298e-09  y:  9.643826e-17\n",
            "step:  947  x:  9.623893e-09  y:  9.2619314e-17\n",
            "step:  948  x:  9.431415e-09  y:  8.895158e-17\n",
            "step:  949  x:  9.242786e-09  y:  8.5429096e-17\n",
            "step:  950  x:  9.05793e-09  y:  8.2046106e-17\n",
            "step:  951  x:  8.876772e-09  y:  7.8797086e-17\n",
            "step:  952  x:  8.699237e-09  y:  7.567672e-17\n",
            "step:  953  x:  8.5252525e-09  y:  7.2679933e-17\n",
            "step:  954  x:  8.354747e-09  y:  6.98018e-17\n",
            "step:  955  x:  8.187652e-09  y:  6.703765e-17\n",
            "step:  956  x:  8.023899e-09  y:  6.4382954e-17\n",
            "step:  957  x:  7.863421e-09  y:  6.183339e-17\n",
            "step:  958  x:  7.706153e-09  y:  5.938479e-17\n",
            "step:  959  x:  7.55203e-09  y:  5.703316e-17\n",
            "step:  960  x:  7.4009896e-09  y:  5.4774646e-17\n",
            "step:  961  x:  7.25297e-09  y:  5.260557e-17\n",
            "step:  962  x:  7.1079103e-09  y:  5.052239e-17\n",
            "step:  963  x:  6.965752e-09  y:  4.85217e-17\n",
            "step:  964  x:  6.8264367e-09  y:  4.660024e-17\n",
            "step:  965  x:  6.689908e-09  y:  4.4754872e-17\n",
            "step:  966  x:  6.55611e-09  y:  4.2982578e-17\n",
            "step:  967  x:  6.4249877e-09  y:  4.1280465e-17\n",
            "step:  968  x:  6.2964878e-09  y:  3.9645758e-17\n",
            "step:  969  x:  6.170558e-09  y:  3.8075785e-17\n",
            "step:  970  x:  6.047147e-09  y:  3.6567984e-17\n",
            "step:  971  x:  5.926204e-09  y:  3.5119895e-17\n",
            "step:  972  x:  5.80768e-09  y:  3.372915e-17\n",
            "step:  973  x:  5.6915264e-09  y:  3.239347e-17\n",
            "step:  974  x:  5.5776956e-09  y:  3.111069e-17\n",
            "step:  975  x:  5.4661418e-09  y:  2.9878707e-17\n",
            "step:  976  x:  5.356819e-09  y:  2.869551e-17\n",
            "step:  977  x:  5.2496825e-09  y:  2.7559167e-17\n",
            "step:  978  x:  5.1446887e-09  y:  2.6467821e-17\n",
            "step:  979  x:  5.041795e-09  y:  2.5419697e-17\n",
            "step:  980  x:  4.940959e-09  y:  2.4413078e-17\n",
            "step:  981  x:  4.84214e-09  y:  2.344632e-17\n",
            "step:  982  x:  4.745297e-09  y:  2.2517845e-17\n",
            "step:  983  x:  4.650391e-09  y:  2.1626137e-17\n",
            "step:  984  x:  4.5573834e-09  y:  2.0769743e-17\n",
            "step:  985  x:  4.466236e-09  y:  1.9947262e-17\n",
            "step:  986  x:  4.3769113e-09  y:  1.9157352e-17\n",
            "step:  987  x:  4.289373e-09  y:  1.839872e-17\n",
            "step:  988  x:  4.2035855e-09  y:  1.7670131e-17\n",
            "step:  989  x:  4.1195136e-09  y:  1.6970393e-17\n",
            "step:  990  x:  4.0371235e-09  y:  1.6298366e-17\n",
            "step:  991  x:  3.956381e-09  y:  1.565295e-17\n",
            "step:  992  x:  3.877253e-09  y:  1.5033093e-17\n",
            "step:  993  x:  3.799708e-09  y:  1.4437782e-17\n",
            "step:  994  x:  3.723714e-09  y:  1.3866046e-17\n",
            "step:  995  x:  3.6492398e-09  y:  1.3316951e-17\n",
            "step:  996  x:  3.576255e-09  y:  1.27896e-17\n",
            "step:  997  x:  3.50473e-09  y:  1.22831325e-17\n",
            "step:  998  x:  3.4346355e-09  y:  1.17967205e-17\n",
            "step:  999  x:  3.3659429e-09  y:  1.1329571e-17\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "使用梯度下降法对函数y=x^2求最小值，设置学习率为0.1\n",
        "'''\n",
        "x = tf.Variable(tf.constant(2.0, dtype=tf.float32), name = 'x')\n",
        "y = x ** 2\n",
        "minimize_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(y)\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as session:\n",
        "  session.run(init)\n",
        "  for i in range(1000):\n",
        "    session.run(minimize_op)\n",
        "    print(\"step: \", i, \" x: \", session.run(x), \" y: \", session.run(y))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "bOOLlMZTaE2I",
        "outputId": "ecfca352-57ed-4717-c617-0f3f52e18890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z =  4.0\n",
            "x =  1.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "'''\n",
        "首先将x 初始化为1，然后有两条语句，第一条作用是实现 x = x + 3, 第二条作用是实现 z = x * 4，\n",
        "如果是串行执行，那么执行第一句后x变成4，执行第二句后z变成16,但tensorflow并不串行执行，\n",
        "所以最终得到结果与上面预想不同\n",
        "'''\n",
        "x = tf.Variable(tf.constant(1.0), name = 'x')\n",
        "x_assign = tf.assign(x, x+3)\n",
        "z = x*4\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as session:\n",
        "  session.run(init)\n",
        "  print('z = ', session.run(z))\n",
        "  print('x = ', session.run(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "CKVWR6shgiPX",
        "outputId": "04566c74-73ff-44f9-b356-6a5a046e8986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z =  20.0\n",
            "x =  5.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "'''\n",
        "让x = x + 3 与 z = x * 4串行执行\n",
        "'''\n",
        "x = tf.Variable(tf.constant(1.0), name = 'x')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as session:\n",
        "  session.run(init)\n",
        "  with tf.control_dependencies([tf.assign(x, x+4)]):\n",
        "    z = x * 4\n",
        "  print('z = ', session.run(z))\n",
        "  print('x = ', session.run(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "I34bzR31pdaQ",
        "outputId": "3d268650-1c84-4846-e318-a18d7f00d55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simple computation result :  31.0\n"
          ]
        }
      ],
      "source": [
        "def  simple_computation(w):\n",
        "  x = tf.Variable(tf.constant(2.0, shape = None, dtype = tf.float32), \n",
        "                  name = 'simple_computation_x')\n",
        "  y = tf.Variable(tf.constant(3.0, shape = None, dtype = tf.float32), \n",
        "                  name = 'simple_computation_y')\n",
        "  z = x * 2 + y**3\n",
        "  return z\n",
        "  \n",
        "with tf.Session() as session:\n",
        "  z = simple_computation(2)\n",
        "  init = tf.global_variables_initializer()\n",
        "  session.run(init)\n",
        "  res = session.run(z)\n",
        "  print('simple computation result : ', res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1088
        },
        "colab_type": "code",
        "id": "7gHxyQmXzHtI",
        "outputId": "de81685e-ecba-44fd-c0bc-84f265609b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simple_computation_x\n",
            "simple_computation_x/Assign\n",
            "simple_computation_x/read\n",
            "simple_computation_x_1\n",
            "simple_computation_x_1/Assign\n",
            "simple_computation_x_1/read\n",
            "simple_computation_x_2\n",
            "simple_computation_x_2/Assign\n",
            "simple_computation_x_2/read\n",
            "simple_computation_x_3\n",
            "simple_computation_x_3/Assign\n",
            "simple_computation_x_3/read\n",
            "simple_computation_x_4\n",
            "simple_computation_x_4/Assign\n",
            "simple_computation_x_4/read\n",
            "simple_computation_x_5\n",
            "simple_computation_x_5/Assign\n",
            "simple_computation_x_5/read\n",
            "simple_computation_x_6\n",
            "simple_computation_x_6/Assign\n",
            "simple_computation_x_6/read\n",
            "simple_computation_x_7\n",
            "simple_computation_x_7/Assign\n",
            "simple_computation_x_7/read\n",
            "simple_computation_x_8\n",
            "simple_computation_x_8/Assign\n",
            "simple_computation_x_8/read\n",
            "simple_computation_x_9\n",
            "simple_computation_x_9/Assign\n",
            "simple_computation_x_9/read\n",
            "simple_computation_x_10\n",
            "simple_computation_x_10/Assign\n",
            "simple_computation_x_10/read\n",
            "simple_computation_x_11\n",
            "simple_computation_x_11/Assign\n",
            "simple_computation_x_11/read\n",
            "simple_computation_x_12\n",
            "simple_computation_x_12/Assign\n",
            "simple_computation_x_12/read\n",
            "simple_computation_x_13\n",
            "simple_computation_x_13/Assign\n",
            "simple_computation_x_13/read\n",
            "simple_computation_x_14\n",
            "simple_computation_x_14/Assign\n",
            "simple_computation_x_14/read\n",
            "simple_computation_x_15\n",
            "simple_computation_x_15/Assign\n",
            "simple_computation_x_15/read\n",
            "simple_computation_x_16\n",
            "simple_computation_x_16/Assign\n",
            "simple_computation_x_16/read\n",
            "simple_computation_x_17\n",
            "simple_computation_x_17/Assign\n",
            "simple_computation_x_17/read\n",
            "simple_computation_x_18\n",
            "simple_computation_x_18/Assign\n",
            "simple_computation_x_18/read\n",
            "simple_computation_x_19\n",
            "simple_computation_x_19/Assign\n",
            "simple_computation_x_19/read\n",
            "simple_computation_x_20\n",
            "simple_computation_x_20/Assign\n",
            "simple_computation_x_20/read\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as session:\n",
        "  for i in range(10):\n",
        "    z = simple_computation(2)\n",
        "    init = tf.global_variables_initializer()\n",
        "    session.run(init)\n",
        "    res = session.run(z)\n",
        "  \n",
        "for n in tf.get_default_graph().as_graph_def().node:\n",
        "  if \"simple_computation_x\" in n.name:\n",
        "    print(n.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EqegQy-sawgP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def  simple_computation_reuse1(w):\n",
        "  #把tf.Variable换成tf.get_variable\n",
        "  x = tf.get_variable('x', initializer = tf.constant(1.0, \n",
        "                                                     shape = None, \n",
        "                                                     dtype = tf.float32))\n",
        "  y = tf.get_variable('y', initializer = tf.constant(2.0,\n",
        "                                                    shape = None,\n",
        "                                                    dtype = tf.float32))\n",
        "  z = x * w + y**2\n",
        "  return z\n",
        "\n",
        "def  simple_computation_reuse2(w):\n",
        "  '''\n",
        "  变量重用时初始化方式必须一致，例如上面变量x初始化值是1.0，重用时它的初始哈值也必须是1.0\n",
        "  '''\n",
        "  x = tf.get_variable('x', initializer = tf.constant(1.0,\n",
        "                                                    shape = None,\n",
        "                                                    dtype = tf.float32))\n",
        "  y = tf.get_variable('y', initializer = tf.constant(2.0,\n",
        "                                                    shape = None,\n",
        "                                                    dtype = tf.float32))\n",
        "  z = w*x*y\n",
        "  return z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DxhJX3NVg_jM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "设定一个命名空间，在该空间里给定名字的变量只分配一次内存，我们可以通过变量名多次获取变量内存，\n",
        "这样可以防止一个变量分配多次内存\n",
        "'''\n",
        "with tf.variable_scope('reuse_scope_exampleA', reuse = tf.AUTO_REUSE) as scope:\n",
        "  #reuse_scope_exampleA/x,reuse_scope_exampleA/y\n",
        "  z1 = simple_computation_reuse1(tf.constant(1.0, dtype = tf.float32))\n",
        "\n",
        "with tf.variable_scope(scope, reuse = tf.AUTO_REUSE) as scope1:\n",
        "  with tf.name_scope(scope1.original_name_scope):\n",
        "    #下面调用会重用reuse_scope_exampleA/x,reuse_scope_exampleA/y\n",
        "    z2 = simple_computation_reuse2(z1)\n",
        "    \n",
        "#再次重用my_reuse_scopeA/x,my_reuse_scopeA/y\n",
        "with tf.variable_scope(scope, reuse = tf.AUTO_REUSE) as scope3:\n",
        "  with tf.name_scope(scope3.original_name_scope):\n",
        "    zz1 = simple_computation_reuse1(tf.constant(1.0, dtype = tf.float32))\n",
        "    zz2 = simple_computation_reuse2(zz1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P3bD2O8dhKak"
      },
      "outputs": [],
      "source": [
        "with tf.variable_scope('reuse_scope_exampleB', reuse = tf.AUTO_REUSE):\n",
        "  #下面调用创建变量reuse_scope_exampleB/x,reuse_scope_exampleB/y\n",
        "  a1 = simple_computation_reuse1(tf.constant(1.0, dtype = tf.float32))\n",
        "with tf.variable_scope('my_reuse_scopeB/', reuse = tf.AUTO_REUSE):\n",
        "  #下面调用重用reuse_scope_exampleB/x, reuse_scope_exampleB/y\n",
        "  a2 = simple_computation_reuse2(a1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "colab_type": "code",
        "id": "v7SY9Q4khghT",
        "outputId": "c7393387-c8e3-489b-f48c-a7a8ca9e55df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reuse_scope_exampleA/Const\n",
            "reuse_scope_exampleA/Const_1\n",
            "reuse_scope_exampleA/x\n",
            "reuse_scope_exampleA/x/Assign\n",
            "reuse_scope_exampleA/x/read\n",
            "reuse_scope_exampleA/Const_2\n",
            "reuse_scope_exampleA/y\n",
            "reuse_scope_exampleA/y/Assign\n",
            "reuse_scope_exampleA/y/read\n",
            "reuse_scope_exampleA/mul\n",
            "reuse_scope_exampleA/pow/y\n",
            "reuse_scope_exampleA/pow\n",
            "reuse_scope_exampleA/add\n",
            "reuse_scope_exampleA/Const_3\n",
            "reuse_scope_exampleA/Const_4\n",
            "reuse_scope_exampleA/mul_1\n",
            "reuse_scope_exampleA/mul_2\n",
            "reuse_scope_exampleA/Const_5\n",
            "reuse_scope_exampleA/Const_6\n",
            "reuse_scope_exampleA/Const_7\n",
            "reuse_scope_exampleA/mul_3\n",
            "reuse_scope_exampleA/pow_1/y\n",
            "reuse_scope_exampleA/pow_1\n",
            "reuse_scope_exampleA/add_1\n",
            "reuse_scope_exampleA/Const_8\n",
            "reuse_scope_exampleA/Const_9\n",
            "reuse_scope_exampleA/mul_4\n",
            "reuse_scope_exampleA/mul_5\n",
            "reuse_scope_exampleB/Const\n",
            "reuse_scope_exampleB/Const_1\n",
            "reuse_scope_exampleB/x\n",
            "reuse_scope_exampleB/x/Assign\n",
            "reuse_scope_exampleB/x/read\n",
            "reuse_scope_exampleB/Const_2\n",
            "reuse_scope_exampleB/y\n",
            "reuse_scope_exampleB/y/Assign\n",
            "reuse_scope_exampleB/y/read\n",
            "reuse_scope_exampleB/mul\n",
            "reuse_scope_exampleB/pow/y\n",
            "reuse_scope_exampleB/pow\n",
            "reuse_scope_exampleB/add\n"
          ]
        }
      ],
      "source": [
        "with  tf.Session() as session:\n",
        "  init = tf.global_variables_initializer()\n",
        "  session.run(init)\n",
        "  res = session.run([z1, z2, z2])\n",
        "    \n",
        "\n",
        "#把含有scope或scopeB的变量打印出，看看他们是否只有一份\n",
        "for n in tf.get_default_graph().as_graph_def().node:\n",
        "  if \"reuse_scope_exampleA\" in n.name or 'reuse_scope_exampleB' in n.name:\n",
        "    print(n.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "colab_type": "code",
        "id": "wvUNleK7GHhd",
        "outputId": "1425b375-5880-4bf6-9b7a-6f203a5ace70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bYI--MzIEe5N"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_num = train_images.shape[0]\n",
        "rows = train_images.shape[1]\n",
        "cols = train_images.shape[2]\n",
        "test_num = test_images.shape[0]\n",
        "\n",
        "train_images = train_images.reshape(train_num, rows * cols)\n",
        "test_images = test_images.reshape(test_num, rows * cols)\n",
        "\n",
        "train_images = (train_images - np.mean(train_images)) / np.std(train_images)\n",
        "test_images = (test_images - np.mean(test_images)) / np.std(test_images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hoHgjVS7GosJ"
      },
      "outputs": [],
      "source": [
        "WEIGHTS = \"weights\"\n",
        "BIAS = \"bias\"\n",
        "\n",
        "batch_size = 100\n",
        "img_width, img_height = 28, 28\n",
        "input_size = img_width * img_height\n",
        "num_labels = 10\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#定义接收图片和图片标签的向量\n",
        "tf_inputs = tf.placeholder(shape=[batch_size, input_size], dtype = tf.float32,\n",
        "                          name = 'inputs')\n",
        "tf_labels = tf.placeholder(shape=[batch_size, num_labels], dtype = tf.float32,\n",
        "                          name = 'labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5PbZ7xRyHwrU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "我们构建的网络有三层，前两层是全连接层，最后一层含有10个节点，每个节点输出当前图片对应数字的概率\n",
        "'''\n",
        "def  define_networks():\n",
        "  with tf.variable_scope('layer1'):\n",
        "    '''\n",
        "    第一层网络有500个节点，由于一张图片相当于28*28的二维数组，因此输入层和第一层网络在全连接\n",
        "    情况下，有(28*28 = 784)*500个链路参数，他们对应一个[784, 500]的二维向量\n",
        "    '''\n",
        "    tf.get_variable(WEIGHTS, shape=[input_size, 500], \n",
        "                    initializer = tf.random_uniform_initializer(0, 0.02))\n",
        "    tf.get_variable(BIAS, shape=[500], \n",
        "                    initializer = tf.random_uniform_initializer(0, 0.01))\n",
        "    \n",
        "  with tf.variable_scope('layer2'):\n",
        "    '''\n",
        "    第二层网络有250个节点，第一层与第二层在全连接情况下有500*250个链路参数，对应[500, 250]\n",
        "    的二维向量\n",
        "    '''\n",
        "    tf.get_variable(WEIGHTS, shape = [500, 250],\n",
        "                   initializer = tf.random_uniform_initializer(0, 0.02))\n",
        "    tf.get_variable(BIAS, shape = [250], \n",
        "                   initializer = tf.random_uniform_initializer(0, 0.01))\n",
        "    \n",
        "  with tf.variable_scope('layer3'):\n",
        "    '''\n",
        "    第三层只有10个节点，第二层与第三层在全连接情况下有250*10个链路参数，对应[250,10]\n",
        "    的二维向量\n",
        "    '''\n",
        "    tf.get_variable(WEIGHTS, shape = [250, 10], \n",
        "                   initializer = tf.random_uniform_initializer(0, 0.02))\n",
        "    tf.get_variable(BIAS, shape = [10],\n",
        "                   initializer = tf.random_uniform_initializer(0, 0.01))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kDB-BhhuK5sk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "设置网络层的激活函数\n",
        "'''\n",
        "def  define_activations(x):\n",
        "  #第一层使用relu激活函数\n",
        "  with tf.variable_scope('layer1', reuse = tf.AUTO_REUSE):\n",
        "    w, b = tf.get_variable(WEIGHTS), tf.get_variable(BIAS)\n",
        "    tf_h1 = tf.nn.relu(tf.matmul(x, w) + b, name = 'hidden1')\n",
        "    \n",
        "  with tf.variable_scope('layer2', reuse = tf.AUTO_REUSE):\n",
        "    #第二层使用relu激活函数\n",
        "    w, b = tf.get_variable(WEIGHTS), tf.get_variable(BIAS)\n",
        "    tf_h2 = tf.nn.relu(tf.matmul(tf_h1, w) + b, name = 'hidden2')\n",
        "    \n",
        "  with tf.variable_scope('layer3', reuse = tf.AUTO_REUSE):\n",
        "    #第三层使用先不使用激活，直接把结果输出\n",
        "    w, b = tf.get_variable(WEIGHTS), tf.get_variable(BIAS)\n",
        "    tf_logits = tf.nn.bias_add(tf.matmul(tf_h2, w), b, name = 'logits')\n",
        "    \n",
        "  return tf_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3jU7uZ3cMVBx"
      },
      "outputs": [],
      "source": [
        "define_networks()\n",
        "\n",
        "logits = define_activations(tf_inputs)\n",
        "#定义损失函数\n",
        "softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = \n",
        "                                                    tf_labels)\n",
        "tf_loss = tf.reduce_mean(softmax)\n",
        "#使用梯度下降法调整网络参数\n",
        "tf_loss_minimize = tf.train.MomentumOptimizer(momentum=0.9, \n",
        "                                              learning_rate = 0.01).minimize(tf_loss)\n",
        "#定义网络对输入图片的判断结果\n",
        "tf_predictions = tf.nn.softmax(define_activations(tf_inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3675
        },
        "colab_type": "code",
        "id": "obyUPV3VQco6",
        "outputId": "1f0e6909-a3af-4fab-a1e8-6deb66a7fb31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one hot labels:\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Average train loss for the 1 epoch: 1.763\n",
            "\n",
            "\tAverage test accuracy for the 1 epoch: 51.28\n",
            "\n",
            "Average train loss for the 2 epoch: 0.837\n",
            "\n",
            "\tAverage test accuracy for the 2 epoch: 85.90\n",
            "\n",
            "Average train loss for the 3 epoch: 0.393\n",
            "\n",
            "\tAverage test accuracy for the 3 epoch: 90.84\n",
            "\n",
            "Average train loss for the 4 epoch: 0.288\n",
            "\n",
            "\tAverage test accuracy for the 4 epoch: 91.62\n",
            "\n",
            "Average train loss for the 5 epoch: 0.244\n",
            "\n",
            "\tAverage test accuracy for the 5 epoch: 92.34\n",
            "\n",
            "Average train loss for the 6 epoch: 0.217\n",
            "\n",
            "\tAverage test accuracy for the 6 epoch: 93.39\n",
            "\n",
            "Average train loss for the 7 epoch: 0.196\n",
            "\n",
            "\tAverage test accuracy for the 7 epoch: 93.95\n",
            "\n",
            "Average train loss for the 8 epoch: 0.178\n",
            "\n",
            "\tAverage test accuracy for the 8 epoch: 94.10\n",
            "\n",
            "Average train loss for the 9 epoch: 0.165\n",
            "\n",
            "\tAverage test accuracy for the 9 epoch: 94.27\n",
            "\n",
            "Average train loss for the 10 epoch: 0.154\n",
            "\n",
            "\tAverage test accuracy for the 10 epoch: 94.18\n",
            "\n",
            "Average train loss for the 11 epoch: 0.145\n",
            "\n",
            "\tAverage test accuracy for the 11 epoch: 94.28\n",
            "\n",
            "Average train loss for the 12 epoch: 0.138\n",
            "\n",
            "\tAverage test accuracy for the 12 epoch: 94.29\n",
            "\n",
            "Average train loss for the 13 epoch: 0.131\n",
            "\n",
            "\tAverage test accuracy for the 13 epoch: 94.19\n",
            "\n",
            "Average train loss for the 14 epoch: 0.125\n",
            "\n",
            "\tAverage test accuracy for the 14 epoch: 93.77\n",
            "\n",
            "Average train loss for the 15 epoch: 0.118\n",
            "\n",
            "\tAverage test accuracy for the 15 epoch: 93.59\n",
            "\n",
            "Average train loss for the 16 epoch: 0.113\n",
            "\n",
            "\tAverage test accuracy for the 16 epoch: 93.74\n",
            "\n",
            "Average train loss for the 17 epoch: 0.107\n",
            "\n",
            "\tAverage test accuracy for the 17 epoch: 93.73\n",
            "\n",
            "Average train loss for the 18 epoch: 0.101\n",
            "\n",
            "\tAverage test accuracy for the 18 epoch: 93.96\n",
            "\n",
            "Average train loss for the 19 epoch: 0.096\n",
            "\n",
            "\tAverage test accuracy for the 19 epoch: 93.97\n",
            "\n",
            "Average train loss for the 20 epoch: 0.091\n",
            "\n",
            "\tAverage test accuracy for the 20 epoch: 93.63\n",
            "\n",
            "Average train loss for the 21 epoch: 0.088\n",
            "\n",
            "\tAverage test accuracy for the 21 epoch: 94.00\n",
            "\n",
            "Average train loss for the 22 epoch: 0.084\n",
            "\n",
            "\tAverage test accuracy for the 22 epoch: 93.86\n",
            "\n",
            "Average train loss for the 23 epoch: 0.081\n",
            "\n",
            "\tAverage test accuracy for the 23 epoch: 93.84\n",
            "\n",
            "Average train loss for the 24 epoch: 0.078\n",
            "\n",
            "\tAverage test accuracy for the 24 epoch: 94.23\n",
            "\n",
            "Average train loss for the 25 epoch: 0.075\n",
            "\n",
            "\tAverage test accuracy for the 25 epoch: 94.20\n",
            "\n",
            "Average train loss for the 26 epoch: 0.072\n",
            "\n",
            "\tAverage test accuracy for the 26 epoch: 94.42\n",
            "\n",
            "Average train loss for the 27 epoch: 0.070\n",
            "\n",
            "\tAverage test accuracy for the 27 epoch: 94.55\n",
            "\n",
            "Average train loss for the 28 epoch: 0.068\n",
            "\n",
            "\tAverage test accuracy for the 28 epoch: 94.82\n",
            "\n",
            "Average train loss for the 29 epoch: 0.065\n",
            "\n",
            "\tAverage test accuracy for the 29 epoch: 95.13\n",
            "\n",
            "Average train loss for the 30 epoch: 0.063\n",
            "\n",
            "\tAverage test accuracy for the 30 epoch: 95.21\n",
            "\n",
            "Average train loss for the 31 epoch: 0.061\n",
            "\n",
            "\tAverage test accuracy for the 31 epoch: 95.46\n",
            "\n",
            "Average train loss for the 32 epoch: 0.058\n",
            "\n",
            "\tAverage test accuracy for the 32 epoch: 95.44\n",
            "\n",
            "Average train loss for the 33 epoch: 0.056\n",
            "\n",
            "\tAverage test accuracy for the 33 epoch: 95.42\n",
            "\n",
            "Average train loss for the 34 epoch: 0.054\n",
            "\n",
            "\tAverage test accuracy for the 34 epoch: 95.44\n",
            "\n",
            "Average train loss for the 35 epoch: 0.052\n",
            "\n",
            "\tAverage test accuracy for the 35 epoch: 95.21\n",
            "\n",
            "Average train loss for the 36 epoch: 0.051\n",
            "\n",
            "\tAverage test accuracy for the 36 epoch: 95.54\n",
            "\n",
            "Average train loss for the 37 epoch: 0.049\n",
            "\n",
            "\tAverage test accuracy for the 37 epoch: 95.49\n",
            "\n",
            "Average train loss for the 38 epoch: 0.047\n",
            "\n",
            "\tAverage test accuracy for the 38 epoch: 95.47\n",
            "\n",
            "Average train loss for the 39 epoch: 0.046\n",
            "\n",
            "\tAverage test accuracy for the 39 epoch: 95.43\n",
            "\n",
            "Average train loss for the 40 epoch: 0.045\n",
            "\n",
            "\tAverage test accuracy for the 40 epoch: 95.46\n",
            "\n",
            "Average train loss for the 41 epoch: 0.043\n",
            "\n",
            "\tAverage test accuracy for the 41 epoch: 95.24\n",
            "\n",
            "Average train loss for the 42 epoch: 0.042\n",
            "\n",
            "\tAverage test accuracy for the 42 epoch: 95.65\n",
            "\n",
            "Average train loss for the 43 epoch: 0.039\n",
            "\n",
            "\tAverage test accuracy for the 43 epoch: 95.33\n",
            "\n",
            "Average train loss for the 44 epoch: 0.038\n",
            "\n",
            "\tAverage test accuracy for the 44 epoch: 95.41\n",
            "\n",
            "Average train loss for the 45 epoch: 0.038\n",
            "\n",
            "\tAverage test accuracy for the 45 epoch: 95.47\n",
            "\n",
            "Average train loss for the 46 epoch: 0.035\n",
            "\n",
            "\tAverage test accuracy for the 46 epoch: 95.50\n",
            "\n",
            "Average train loss for the 47 epoch: 0.036\n",
            "\n",
            "\tAverage test accuracy for the 47 epoch: 95.64\n",
            "\n",
            "Average train loss for the 48 epoch: 0.036\n",
            "\n",
            "\tAverage test accuracy for the 48 epoch: 95.61\n",
            "\n",
            "Average train loss for the 49 epoch: 0.034\n",
            "\n",
            "\tAverage test accuracy for the 49 epoch: 95.73\n",
            "\n",
            "Average train loss for the 50 epoch: 0.034\n",
            "\n",
            "\tAverage test accuracy for the 50 epoch: 95.69\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#启动训练流程\n",
        "session = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "def  accuracy(predictions, labels):\n",
        "  #统计判断结果的准确率\n",
        "  return np.sum(np.argmax(predictions, axis = 1).flatten() == labels.flatten()) / batch_size\n",
        "\n",
        "test_accuracy_over_time = []\n",
        "train_loss_over_time = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss = []\n",
        "  for step in range(train_images.shape[0] // batch_size):\n",
        "    #将标签转换为One-hot-vector\n",
        "    labels_one_hot = np.zeros((batch_size, num_labels), dtype = np.float32)\n",
        "    labels_one_hot[np.arange(batch_size), \n",
        "                   train_labels[step*batch_size: (step+1)*batch_size]] = 1.0\n",
        "    \n",
        "    if epoch == 0 and step == 0:\n",
        "      print('one hot labels:')\n",
        "      print(labels_one_hot[:10])\n",
        "      print()\n",
        "    \n",
        "    #执行训练流程\n",
        "    loss, _ = session.run([tf_loss, tf_loss_minimize], feed_dict = {\n",
        "        tf_inputs: train_images[step*batch_size : (step+1)*batch_size, :],\n",
        "        tf_labels: labels_one_hot\n",
        "    })\n",
        "    train_loss.append(loss)\n",
        "  \n",
        "  test_accuracy = []\n",
        "  #测试网络训练效果\n",
        "  for step in range(test_images.shape[0] // batch_size):\n",
        "    test_predictions = session.run(tf_predictions ,\n",
        "                                   feed_dict = {tf_inputs: \n",
        "                                   test_images[step*batch_size : (step+1)*batch_size, :]})\n",
        "    \n",
        "    batch_test_accuracy = accuracy(test_predictions, test_labels[step*batch_size:\n",
        "                                                                (step+1)*batch_size])\n",
        "    test_accuracy.append(batch_test_accuracy)\n",
        "  \n",
        "  print(\"Average train loss for the %d epoch: %.3f\\n\" % (epoch+1, np.mean(train_loss)))\n",
        "  train_loss_over_time.append(np.mean(train_loss))\n",
        "  print('\\tAverage test accuracy for the %d epoch: %.2f\\n' % (epoch+1, np.mean(test_accuracy) * 100.0))\n",
        "  test_accuracy_over_time.append(np.mean(test_accuracy)*100)\n",
        "  \n",
        "session.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "colab_type": "code",
        "id": "oqg1FdzZj7Jc",
        "outputId": "e7fe3a8c-efaf-44b5-fe9a-6fb64d39b8a0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd4cea4bc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_axis = np.arange(len(train_loss_over_time))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "fig.set_size_inches(w=25,h=5)\n",
        "ax[0].plot(x_axis, train_loss_over_time)\n",
        "ax[0].set_xlabel('Epochs',fontsize=18)\n",
        "ax[0].set_ylabel('Average train loss',fontsize=18)\n",
        "ax[0].set_title('Training Loss over Time',fontsize=20)\n",
        "ax[1].plot(x_axis, test_accuracy_over_time)\n",
        "ax[1].set_xlabel('Epochs',fontsize=18)\n",
        "ax[1].set_ylabel('Test accuracy',fontsize=18)\n",
        "ax[1].set_title('Test Accuracy over Time',fontsize=20)\n",
        "fig.savefig('mnist_stats.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "第11章.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
