// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/predict.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "tensorflow_serving/apis/predict.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)

namespace tensorflow {
namespace serving {

namespace {

const ::google::protobuf::Descriptor* PredictRequest_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  PredictRequest_reflection_ = NULL;
const ::google::protobuf::Descriptor* PredictRequest_InputsEntry_descriptor_ = NULL;
const ::google::protobuf::Descriptor* PredictResponse_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  PredictResponse_reflection_ = NULL;
const ::google::protobuf::Descriptor* PredictResponse_OutputsEntry_descriptor_ = NULL;

}  // namespace


void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto() {
  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  const ::google::protobuf::FileDescriptor* file =
    ::google::protobuf::DescriptorPool::generated_pool()->FindFileByName(
      "tensorflow_serving/apis/predict.proto");
  GOOGLE_CHECK(file != NULL);
  PredictRequest_descriptor_ = file->message_type(0);
  static const int PredictRequest_offsets_[3] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictRequest, model_spec_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictRequest, inputs_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictRequest, output_filter_),
  };
  PredictRequest_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      PredictRequest_descriptor_,
      PredictRequest::internal_default_instance(),
      PredictRequest_offsets_,
      -1,
      -1,
      -1,
      sizeof(PredictRequest),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictRequest, _internal_metadata_));
  PredictRequest_InputsEntry_descriptor_ = PredictRequest_descriptor_->nested_type(0);
  PredictResponse_descriptor_ = file->message_type(1);
  static const int PredictResponse_offsets_[1] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictResponse, outputs_),
  };
  PredictResponse_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      PredictResponse_descriptor_,
      PredictResponse::internal_default_instance(),
      PredictResponse_offsets_,
      -1,
      -1,
      -1,
      sizeof(PredictResponse),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictResponse, _internal_metadata_));
  PredictResponse_OutputsEntry_descriptor_ = PredictResponse_descriptor_->nested_type(0);
}

namespace {

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AssignDescriptors_once_);
void protobuf_AssignDescriptorsOnce() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AssignDescriptors_once_,
                 &protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      PredictRequest_descriptor_, PredictRequest::internal_default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
        PredictRequest_InputsEntry_descriptor_,
        ::google::protobuf::internal::MapEntry<
            ::std::string,
            ::tensorflow::TensorProto,
            ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
            ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
            0>::CreateDefaultInstance(
                PredictRequest_InputsEntry_descriptor_));
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      PredictResponse_descriptor_, PredictResponse::internal_default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
        PredictResponse_OutputsEntry_descriptor_,
        ::google::protobuf::internal::MapEntry<
            ::std::string,
            ::tensorflow::TensorProto,
            ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
            ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
            0>::CreateDefaultInstance(
                PredictResponse_OutputsEntry_descriptor_));
}

}  // namespace

void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fpredict_2eproto() {
  PredictRequest_default_instance_.Shutdown();
  delete PredictRequest_reflection_;
  PredictResponse_default_instance_.Shutdown();
  delete PredictResponse_reflection_;
}

void protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto_impl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::tensorflow::protobuf_InitDefaults_tensorflow_2fcore_2fframework_2ftensor_2eproto();
  ::tensorflow::serving::protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fmodel_2eproto();
  ::google::protobuf::internal::GetEmptyString();
  PredictRequest_default_instance_.DefaultConstruct();
  ::google::protobuf::internal::GetEmptyString();
  PredictResponse_default_instance_.DefaultConstruct();
  PredictRequest_default_instance_.get_mutable()->InitAsDefaultInstance();
  PredictResponse_default_instance_.get_mutable()->InitAsDefaultInstance();
}

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto_once_);
void protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto() {
  ::google::protobuf::GoogleOnceInit(&protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto_once_,
                 &protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto_impl);
}
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto_impl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
    "\n%tensorflow_serving/apis/predict.proto\022"
    "\022tensorflow.serving\032&tensorflow/core/fra"
    "mework/tensor.proto\032#tensorflow_serving/"
    "apis/model.proto\"\342\001\n\016PredictRequest\0221\n\nm"
    "odel_spec\030\001 \001(\0132\035.tensorflow.serving.Mod"
    "elSpec\022>\n\006inputs\030\002 \003(\0132..tensorflow.serv"
    "ing.PredictRequest.InputsEntry\022\025\n\routput"
    "_filter\030\003 \003(\t\032F\n\013InputsEntry\022\013\n\003key\030\001 \001("
    "\t\022&\n\005value\030\002 \001(\0132\027.tensorflow.TensorProt"
    "o:\0028\001\"\235\001\n\017PredictResponse\022A\n\007outputs\030\001 \003"
    "(\01320.tensorflow.serving.PredictResponse."
    "OutputsEntry\032G\n\014OutputsEntry\022\013\n\003key\030\001 \001("
    "\t\022&\n\005value\030\002 \001(\0132\027.tensorflow.TensorProt"
    "o:\0028\001B\003\370\001\001b\006proto3", 538);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "tensorflow_serving/apis/predict.proto", &protobuf_RegisterTypes);
  ::tensorflow::protobuf_AddDesc_tensorflow_2fcore_2fframework_2ftensor_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fmodel_2eproto();
  ::google::protobuf::internal::OnShutdown(&protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fpredict_2eproto);
}

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto_once_);
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto_once_,
                 &protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto_impl);
}
// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fpredict_2eproto {
  StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fpredict_2eproto() {
    protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  }
} static_descriptor_initializer_tensorflow_5fserving_2fapis_2fpredict_2eproto_;

namespace {

static void MergeFromFail(int line) GOOGLE_ATTRIBUTE_COLD GOOGLE_ATTRIBUTE_NORETURN;
static void MergeFromFail(int line) {
  ::google::protobuf::internal::MergeFromFail(__FILE__, line);
}

}  // namespace


// ===================================================================

void PredictRequest::_slow_mutable_model_spec() {
  model_spec_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ModelSpec* PredictRequest::_slow_release_model_spec() {
  if (model_spec_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ModelSpec* temp = new ::tensorflow::serving::ModelSpec(*model_spec_);
    model_spec_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ModelSpec* PredictRequest::unsafe_arena_release_model_spec() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictRequest.model_spec)
  
  ::tensorflow::serving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
void PredictRequest::_slow_set_allocated_model_spec(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*model_spec) == NULL) {
      message_arena->Own(*model_spec);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*model_spec)) {
      ::tensorflow::serving::ModelSpec* new_model_spec = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
            message_arena);
      new_model_spec->CopyFrom(**model_spec);
      *model_spec = new_model_spec;
    }
}
void PredictRequest::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == NULL) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictRequest.model_spec)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictRequest::kModelSpecFieldNumber;
const int PredictRequest::kInputsFieldNumber;
const int PredictRequest::kOutputFilterFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictRequest::PredictRequest()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictRequest)
}
PredictRequest::PredictRequest(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  inputs_(arena),
  output_filter_(arena) {
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
  protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictRequest)
}

void PredictRequest::InitAsDefaultInstance() {
  model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(
      ::tensorflow::serving::ModelSpec::internal_default_instance());
}

PredictRequest::PredictRequest(const PredictRequest& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictRequest)
}

void PredictRequest::SharedCtor() {
  inputs_.SetAssignDescriptorCallback(
      protobuf_AssignDescriptorsOnce);
  inputs_.SetEntryDescriptor(
      &::tensorflow::serving::PredictRequest_InputsEntry_descriptor_);
  model_spec_ = NULL;
  _cached_size_ = 0;
}

PredictRequest::~PredictRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictRequest)
  SharedDtor();
}

void PredictRequest::SharedDtor() {
  ::google::protobuf::Arena* arena = GetArenaNoVirtual();
  if (arena != NULL) {
    return;
  }

  if (this != &PredictRequest_default_instance_.get()) {
    delete model_spec_;
  }
}

void PredictRequest::ArenaDtor(void* object) {
  PredictRequest* _this = reinterpret_cast< PredictRequest* >(object);
  (void)_this;
}
void PredictRequest::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void PredictRequest::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* PredictRequest::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return PredictRequest_descriptor_;
}

const PredictRequest& PredictRequest::default_instance() {
  protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PredictRequest> PredictRequest_default_instance_;

PredictRequest* PredictRequest::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<PredictRequest>(arena);
}

void PredictRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictRequest)
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
  inputs_.Clear();
  output_filter_.Clear();
}

bool PredictRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictRequest)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_inputs;
        break;
      }

      // map<string, .tensorflow.TensorProto> inputs = 2;
      case 2: {
        if (tag == 18) {
         parse_inputs:
          DO_(input->IncrementRecursionDepth());
         parse_loop_inputs:
          PredictRequest_InputsEntry::Parser< ::google::protobuf::internal::MapField<
              ::std::string, ::tensorflow::TensorProto,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto > > parser(&inputs_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), parser.key().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictRequest.InputsEntry.key"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_loop_inputs;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(26)) goto parse_output_filter;
        break;
      }

      // repeated string output_filter = 3;
      case 3: {
        if (tag == 26) {
         parse_output_filter:
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output_filter()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output_filter(this->output_filter_size() - 1).data(),
            this->output_filter(this->output_filter_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictRequest.output_filter"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_output_filter;
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictRequest)
  return false;
#undef DO_
}

void PredictRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictRequest)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->model_spec_, output);
  }

  // map<string, .tensorflow.TensorProto> inputs = 2;
  if (!this->inputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictRequest.InputsEntry.key");
      }
    };

    if (output->IsSerializationDeterminstic() &&
        this->inputs().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->inputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<PredictRequest_InputsEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(inputs_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            2, *entry, output);
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<PredictRequest_InputsEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it) {
        entry.reset(inputs_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            2, *entry, output);
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(&*it);
      }
    }
  }

  // repeated string output_filter = 3;
  for (int i = 0; i < this->output_filter_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output_filter(i).data(), this->output_filter(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.PredictRequest.output_filter");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output_filter(i), output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictRequest)
}

::google::protobuf::uint8* PredictRequest::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictRequest)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->model_spec_, false, target);
  }

  // map<string, .tensorflow.TensorProto> inputs = 2;
  if (!this->inputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictRequest.InputsEntry.key");
      }
    };

    if (deterministic &&
        this->inputs().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->inputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<PredictRequest_InputsEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(inputs_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       2, *entry, deterministic, target);
;
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<PredictRequest_InputsEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->inputs().begin();
          it != this->inputs().end(); ++it) {
        entry.reset(inputs_.NewEntryWrapper(
            it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       2, *entry, deterministic, target);
;
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(&*it);
      }
    }
  }

  // repeated string output_filter = 3;
  for (int i = 0; i < this->output_filter_size(); i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output_filter(i).data(), this->output_filter(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.PredictRequest.output_filter");
    target = ::google::protobuf::internal::WireFormatLite::
      WriteStringToArray(3, this->output_filter(i), target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictRequest)
  return target;
}

size_t PredictRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictRequest)
  size_t total_size = 0;

  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->model_spec_);
  }

  // map<string, .tensorflow.TensorProto> inputs = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->inputs_size());
  {
    ::google::protobuf::scoped_ptr<PredictRequest_InputsEntry> entry;
    for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
        it = this->inputs().begin();
        it != this->inputs().end(); ++it) {
      if (entry.get() != NULL && entry->GetArena() != NULL) {
        entry.release();
      }
      entry.reset(inputs_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
    if (entry.get() != NULL && entry->GetArena() != NULL) {
      entry.release();
    }
  }

  // repeated string output_filter = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_filter_size());
  for (int i = 0; i < this->output_filter_size(); i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output_filter(i));
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PredictRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictRequest)
  if (GOOGLE_PREDICT_FALSE(&from == this)) MergeFromFail(__LINE__);
  const PredictRequest* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictRequest>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictRequest)
    UnsafeMergeFrom(*source);
  }
}

void PredictRequest::MergeFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictRequest)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PredictRequest::UnsafeMergeFrom(const PredictRequest& from) {
  GOOGLE_DCHECK(&from != this);
  inputs_.MergeFrom(from.inputs_);
  output_filter_.UnsafeMergeFrom(from.output_filter_);
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
}

void PredictRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictRequest::CopyFrom(const PredictRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictRequest)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PredictRequest::IsInitialized() const {

  return true;
}

void PredictRequest::Swap(PredictRequest* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictRequest temp;
    temp.UnsafeMergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void PredictRequest::UnsafeArenaSwap(PredictRequest* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictRequest::InternalSwap(PredictRequest* other) {
  std::swap(model_spec_, other->model_spec_);
  inputs_.Swap(&other->inputs_);
  output_filter_.UnsafeArenaSwap(&other->output_filter_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata PredictRequest::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = PredictRequest_descriptor_;
  metadata.reflection = PredictRequest_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PredictRequest

// optional .tensorflow.serving.ModelSpec model_spec = 1;
bool PredictRequest::has_model_spec() const {
  return this != internal_default_instance() && model_spec_ != NULL;
}
void PredictRequest::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
const ::tensorflow::serving::ModelSpec& PredictRequest::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictRequest.model_spec)
  return model_spec_ != NULL ? *model_spec_
                         : *::tensorflow::serving::ModelSpec::internal_default_instance();
}
::tensorflow::serving::ModelSpec* PredictRequest::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictRequest.model_spec)
  return model_spec_;
}
::tensorflow::serving::ModelSpec* PredictRequest::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictRequest.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
 void PredictRequest::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictRequest.model_spec)
}

// map<string, .tensorflow.TensorProto> inputs = 2;
int PredictRequest::inputs_size() const {
  return inputs_.size();
}
void PredictRequest::clear_inputs() {
  inputs_.Clear();
}
 const ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >&
PredictRequest::inputs() const {
  // @@protoc_insertion_point(field_map:tensorflow.serving.PredictRequest.inputs)
  return inputs_.GetMap();
}
 ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >*
PredictRequest::mutable_inputs() {
  // @@protoc_insertion_point(field_mutable_map:tensorflow.serving.PredictRequest.inputs)
  return inputs_.MutableMap();
}

// repeated string output_filter = 3;
int PredictRequest::output_filter_size() const {
  return output_filter_.size();
}
void PredictRequest::clear_output_filter() {
  output_filter_.Clear();
}
const ::std::string& PredictRequest::output_filter(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictRequest.output_filter)
  return output_filter_.Get(index);
}
::std::string* PredictRequest::mutable_output_filter(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictRequest.output_filter)
  return output_filter_.Mutable(index);
}
void PredictRequest::set_output_filter(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:tensorflow.serving.PredictRequest.output_filter)
  output_filter_.Mutable(index)->assign(value);
}
void PredictRequest::set_output_filter(int index, const char* value) {
  output_filter_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:tensorflow.serving.PredictRequest.output_filter)
}
void PredictRequest::set_output_filter(int index, const char* value, size_t size) {
  output_filter_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:tensorflow.serving.PredictRequest.output_filter)
}
::std::string* PredictRequest::add_output_filter() {
  // @@protoc_insertion_point(field_add_mutable:tensorflow.serving.PredictRequest.output_filter)
  return output_filter_.Add();
}
void PredictRequest::add_output_filter(const ::std::string& value) {
  output_filter_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:tensorflow.serving.PredictRequest.output_filter)
}
void PredictRequest::add_output_filter(const char* value) {
  output_filter_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:tensorflow.serving.PredictRequest.output_filter)
}
void PredictRequest::add_output_filter(const char* value, size_t size) {
  output_filter_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:tensorflow.serving.PredictRequest.output_filter)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
PredictRequest::output_filter() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.PredictRequest.output_filter)
  return output_filter_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
PredictRequest::mutable_output_filter() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.PredictRequest.output_filter)
  return &output_filter_;
}

inline const PredictRequest* PredictRequest::internal_default_instance() {
  return &PredictRequest_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictResponse::kOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictResponse::PredictResponse()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  if (this != internal_default_instance()) protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictResponse)
}
PredictResponse::PredictResponse(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  outputs_(arena) {
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
  protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictResponse)
}

void PredictResponse::InitAsDefaultInstance() {
}

PredictResponse::PredictResponse(const PredictResponse& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  UnsafeMergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictResponse)
}

void PredictResponse::SharedCtor() {
  outputs_.SetAssignDescriptorCallback(
      protobuf_AssignDescriptorsOnce);
  outputs_.SetEntryDescriptor(
      &::tensorflow::serving::PredictResponse_OutputsEntry_descriptor_);
  _cached_size_ = 0;
}

PredictResponse::~PredictResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictResponse)
  SharedDtor();
}

void PredictResponse::SharedDtor() {
  ::google::protobuf::Arena* arena = GetArenaNoVirtual();
  if (arena != NULL) {
    return;
  }

}

void PredictResponse::ArenaDtor(void* object) {
  PredictResponse* _this = reinterpret_cast< PredictResponse* >(object);
  (void)_this;
}
void PredictResponse::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void PredictResponse::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* PredictResponse::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return PredictResponse_descriptor_;
}

const PredictResponse& PredictResponse::default_instance() {
  protobuf_InitDefaults_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  return *internal_default_instance();
}

::google::protobuf::internal::ExplicitlyConstructed<PredictResponse> PredictResponse_default_instance_;

PredictResponse* PredictResponse::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<PredictResponse>(arena);
}

void PredictResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictResponse)
  outputs_.Clear();
}

bool PredictResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictResponse)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // map<string, .tensorflow.TensorProto> outputs = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_outputs:
          PredictResponse_OutputsEntry::Parser< ::google::protobuf::internal::MapField<
              ::std::string, ::tensorflow::TensorProto,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto > > parser(&outputs_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), parser.key().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.PredictResponse.OutputsEntry.key"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_outputs;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictResponse)
  return false;
#undef DO_
}

void PredictResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictResponse)
  // map<string, .tensorflow.TensorProto> outputs = 1;
  if (!this->outputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictResponse.OutputsEntry.key");
      }
    };

    if (output->IsSerializationDeterminstic() &&
        this->outputs().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->outputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<PredictResponse_OutputsEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(outputs_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            1, *entry, output);
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<PredictResponse_OutputsEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it) {
        entry.reset(outputs_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            1, *entry, output);
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(&*it);
      }
    }
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictResponse)
}

::google::protobuf::uint8* PredictResponse::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictResponse)
  // map<string, .tensorflow.TensorProto> outputs = 1;
  if (!this->outputs().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.PredictResponse.OutputsEntry.key");
      }
    };

    if (deterministic &&
        this->outputs().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->outputs().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<PredictResponse_OutputsEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(outputs_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       1, *entry, deterministic, target);
;
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<PredictResponse_OutputsEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
          it = this->outputs().begin();
          it != this->outputs().end(); ++it) {
        entry.reset(outputs_.NewEntryWrapper(
            it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       1, *entry, deterministic, target);
;
        if (entry->GetArena() != NULL) {
          entry.release();
        }
        Utf8Check::Check(&*it);
      }
    }
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictResponse)
  return target;
}

size_t PredictResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictResponse)
  size_t total_size = 0;

  // map<string, .tensorflow.TensorProto> outputs = 1;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->outputs_size());
  {
    ::google::protobuf::scoped_ptr<PredictResponse_OutputsEntry> entry;
    for (::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >::const_iterator
        it = this->outputs().begin();
        it != this->outputs().end(); ++it) {
      if (entry.get() != NULL && entry->GetArena() != NULL) {
        entry.release();
      }
      entry.reset(outputs_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
    if (entry.get() != NULL && entry->GetArena() != NULL) {
      entry.release();
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PredictResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictResponse)
  if (GOOGLE_PREDICT_FALSE(&from == this)) MergeFromFail(__LINE__);
  const PredictResponse* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictResponse>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictResponse)
    UnsafeMergeFrom(*source);
  }
}

void PredictResponse::MergeFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictResponse)
  if (GOOGLE_PREDICT_TRUE(&from != this)) {
    UnsafeMergeFrom(from);
  } else {
    MergeFromFail(__LINE__);
  }
}

void PredictResponse::UnsafeMergeFrom(const PredictResponse& from) {
  GOOGLE_DCHECK(&from != this);
  outputs_.MergeFrom(from.outputs_);
}

void PredictResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictResponse::CopyFrom(const PredictResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictResponse)
  if (&from == this) return;
  Clear();
  UnsafeMergeFrom(from);
}

bool PredictResponse::IsInitialized() const {

  return true;
}

void PredictResponse::Swap(PredictResponse* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictResponse temp;
    temp.UnsafeMergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void PredictResponse::UnsafeArenaSwap(PredictResponse* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictResponse::InternalSwap(PredictResponse* other) {
  outputs_.Swap(&other->outputs_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata PredictResponse::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = PredictResponse_descriptor_;
  metadata.reflection = PredictResponse_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PredictResponse

// map<string, .tensorflow.TensorProto> outputs = 1;
int PredictResponse::outputs_size() const {
  return outputs_.size();
}
void PredictResponse::clear_outputs() {
  outputs_.Clear();
}
 const ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >&
PredictResponse::outputs() const {
  // @@protoc_insertion_point(field_map:tensorflow.serving.PredictResponse.outputs)
  return outputs_.GetMap();
}
 ::google::protobuf::Map< ::std::string, ::tensorflow::TensorProto >*
PredictResponse::mutable_outputs() {
  // @@protoc_insertion_point(field_mutable_map:tensorflow.serving.PredictResponse.outputs)
  return outputs_.MutableMap();
}

inline const PredictResponse* PredictResponse::internal_default_instance() {
  return &PredictResponse_default_instance_.get();
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)
